{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":".. evolving a design paradigm for hypertext based web service API Welcome to meshcaline.org! This blog contains a series of articles around meshcaline , a design paradigm for hypertext based web service API. Although the ideas of the meshcaline design are appropriate for many classes of API, it creates most value for API that are directly called from end-user devices (browser, mobile app) and that enable end-user facing business process implementations REST in Piece Everybody claims to have REST API theses days. Is this correct? And is REST really the right design paradigm for APIs? Analyzing Roy Fielding's rules for REST APIs raises many concerns for the applicability of REST (as defined by the inventor) for public service API offerings. ...more Basic meshcaline What are the fundamental concepts and design paradigms of meshcaline and how does it implement hypertext? An introduction for getting started with meshcaline and a must read that sets the scene for most of the other posts. ...more Why Hypertext API? Despite the success of the hypertext concept in the browsealbe web, in the programmable web of service API, hypertext is only rarely used. Read how API users, API developers and API business benefit from a hypertext based service API. ...more API Permalinks Individual resources in a meshcaline API represent steps that guide an end-user through a business process. As such they are by default temporary resources. The simple permalink pattern enables API users to keep references to steps from one business process and start new business processes from there in subsequent sessions. ...more Paginateable Collections Collections of objects with potentially large number of items risk that the size of resource representation grows beyond a reasonable limit. To avoid that risk meshcaline recommends a standard representation pattern for paginateable collections. ...more API Evolution & Compatibility Now that we have the basic ingredients to design the API for our product, let's analyze what we can do to iteratively evolve our product while at the same time provide a predictable and reliable service to our clients. ...more Consumer Driven Response Types By combining ideas of consumer driven contracts and response filtering meshcaline allows you to better understand how your clients use your API and with that enable you to control experimental features and to manage the introduction of incompatible changes ...more","title":"Home"},{"location":"#evolving-a-design-paradigm-for-hypertext-based-web-service-api","text":"","title":".. evolving a design paradigm for hypertext based web service API"},{"location":"#welcome-to-meshcalineorg","text":"This blog contains a series of articles around meshcaline , a design paradigm for hypertext based web service API. Although the ideas of the meshcaline design are appropriate for many classes of API, it creates most value for API that are directly called from end-user devices (browser, mobile app) and that enable end-user facing business process implementations","title":"Welcome to meshcaline.org!"},{"location":"#rest-in-piece","text":"Everybody claims to have REST API theses days. Is this correct? And is REST really the right design paradigm for APIs? Analyzing Roy Fielding's rules for REST APIs raises many concerns for the applicability of REST (as defined by the inventor) for public service API offerings. ...more","title":"REST in Piece"},{"location":"#basic-meshcaline","text":"What are the fundamental concepts and design paradigms of meshcaline and how does it implement hypertext? An introduction for getting started with meshcaline and a must read that sets the scene for most of the other posts. ...more","title":"Basic meshcaline"},{"location":"#why-hypertext-api","text":"Despite the success of the hypertext concept in the browsealbe web, in the programmable web of service API, hypertext is only rarely used. Read how API users, API developers and API business benefit from a hypertext based service API. ...more","title":"Why Hypertext API?"},{"location":"#api-permalinks","text":"Individual resources in a meshcaline API represent steps that guide an end-user through a business process. As such they are by default temporary resources. The simple permalink pattern enables API users to keep references to steps from one business process and start new business processes from there in subsequent sessions. ...more","title":"API Permalinks"},{"location":"#paginateable-collections","text":"Collections of objects with potentially large number of items risk that the size of resource representation grows beyond a reasonable limit. To avoid that risk meshcaline recommends a standard representation pattern for paginateable collections. ...more","title":"Paginateable Collections"},{"location":"#api-evolution-compatibility","text":"Now that we have the basic ingredients to design the API for our product, let's analyze what we can do to iteratively evolve our product while at the same time provide a predictable and reliable service to our clients. ...more","title":"API Evolution &amp; Compatibility"},{"location":"#consumer-driven-response-types","text":"By combining ideas of consumer driven contracts and response filtering meshcaline allows you to better understand how your clients use your API and with that enable you to control experimental features and to manage the introduction of incompatible changes ...more","title":"Consumer Driven Response Types"},{"location":"about/","text":"About this blog Working on a public web service API for some years now, I (Andreas Schmidt) thought that posting a series of articles on the experiences made and conclusions/solutions derived from that might be worth to share with a wider audience. Hopefully this blog gives you some inspiration for your own API project and helps you avoiding some of the design iteration my team and I went through. Scope You can build web service API for many different purposes. Each purpose comes with various aspects beyond the specific functionality it is supposed to deliver. In most cases other aspects will also have a massive influence on the question how good a given design serves the purpose of the API, e.g.: Business : How can I monetize the API offering? Audience : For whom do you provide the API? How will those users of the API want, need, or expect to use the API? Client platforms : How will clients connect to the web service API? What are the capabilities of those client platforms? In this blog I will focus on solutions for a very specific subset of API purposes, simply because it is the domain I've been focusing on during the last years: Public API offerings, where the API provides access to a commercial offering that is expected to be called directly from within a client application on the end-user's device. In this domain several aspects play a very important role: Public APIs get developed for an anonymous audience. You might have a closer relationship with some of your users, but the majority you will never meet f2f or have any direct communication with. Most developers will learn about your offering through some form of communications and then spend some time evaluating your product. If they get into your API easily and find quality and terms appropriate you have a chance to get a new customer. If not, there is a high probability that you will never receive any feedback from them and therefore have no chance to learn what they disliked. A commercial product competes on the market. For this it needs differentiation. Assuming that monetization of a commercial API is based on its usage, you try to bind customers as long as possible and motivate them to use your product more often. Keep in mind that this to a certain extent contradicts with your user's goals: While they might love your solution, \"provider lock-in\" is something that most customers don't like. That means, you must find the right balance. Any remote call has an impact on the performance characteristics of the calling application. API accessed directly from client applications need to take this into account, especially when those connect through mobile networks. In addition the API must be able to handle limited capabilities that specific client platforms may have. You might have already realized that there is no \"silver bullet\" for API design. A given design paradigm might be the ideal solution for one purpose, but may result in a complete disaster if applied for a different one. So whenever you find some popular API you like, just copying its design paradigm into your domain won\u2019t automatically make your API useful, loved and popular as well. At the same time, copy-with-pride can be a very good practice in API design, as it can dramatically reduce the learning curve of your potential users. Particularly when you compete with other solutions in a given domain, this may be the significant reason, why developers chose your API for further evaluation instead of another one: they simply feel already familiar with the API from the start. Therefore you not only need to understand the building blocks of an API design you like, you also have to understand which aspects of the purpose of a given API they support. While this blog is about a technical domain, quite often I will give non-technical reasons why I believe the proposed solution is worth to consider. I assume that many of the topics I will touch in this blog can also help API designers in other domains, and I would love to receive your feedback on that. Be warned Not all of the concepts described in these articles did find their way into the API I've been working on and by that had a chance to proof their value in the field. So better don't count on everything being perfect. If you find room for improvement, or confusing (hopefully not contradicting) aspects, please drop a comment on the post if( window.location.pathname==\"/about/\" ){ var t=\"org\", d=\"meshcaline\", n=\"meshcalero\"; var m=n+\"@\"+d+\".\"+t; document.write('or send me an email to <a href=\"mailto:'+m+'\">'+m+'</a>');} About me If you're interested to know more about me you might want to visit my Linkedin Profile as a starting point.","title":"About"},{"location":"about/#about-this-blog","text":"Working on a public web service API for some years now, I (Andreas Schmidt) thought that posting a series of articles on the experiences made and conclusions/solutions derived from that might be worth to share with a wider audience. Hopefully this blog gives you some inspiration for your own API project and helps you avoiding some of the design iteration my team and I went through.","title":"About this blog"},{"location":"about/#scope","text":"You can build web service API for many different purposes. Each purpose comes with various aspects beyond the specific functionality it is supposed to deliver. In most cases other aspects will also have a massive influence on the question how good a given design serves the purpose of the API, e.g.: Business : How can I monetize the API offering? Audience : For whom do you provide the API? How will those users of the API want, need, or expect to use the API? Client platforms : How will clients connect to the web service API? What are the capabilities of those client platforms? In this blog I will focus on solutions for a very specific subset of API purposes, simply because it is the domain I've been focusing on during the last years: Public API offerings, where the API provides access to a commercial offering that is expected to be called directly from within a client application on the end-user's device. In this domain several aspects play a very important role: Public APIs get developed for an anonymous audience. You might have a closer relationship with some of your users, but the majority you will never meet f2f or have any direct communication with. Most developers will learn about your offering through some form of communications and then spend some time evaluating your product. If they get into your API easily and find quality and terms appropriate you have a chance to get a new customer. If not, there is a high probability that you will never receive any feedback from them and therefore have no chance to learn what they disliked. A commercial product competes on the market. For this it needs differentiation. Assuming that monetization of a commercial API is based on its usage, you try to bind customers as long as possible and motivate them to use your product more often. Keep in mind that this to a certain extent contradicts with your user's goals: While they might love your solution, \"provider lock-in\" is something that most customers don't like. That means, you must find the right balance. Any remote call has an impact on the performance characteristics of the calling application. API accessed directly from client applications need to take this into account, especially when those connect through mobile networks. In addition the API must be able to handle limited capabilities that specific client platforms may have. You might have already realized that there is no \"silver bullet\" for API design. A given design paradigm might be the ideal solution for one purpose, but may result in a complete disaster if applied for a different one. So whenever you find some popular API you like, just copying its design paradigm into your domain won\u2019t automatically make your API useful, loved and popular as well. At the same time, copy-with-pride can be a very good practice in API design, as it can dramatically reduce the learning curve of your potential users. Particularly when you compete with other solutions in a given domain, this may be the significant reason, why developers chose your API for further evaluation instead of another one: they simply feel already familiar with the API from the start. Therefore you not only need to understand the building blocks of an API design you like, you also have to understand which aspects of the purpose of a given API they support. While this blog is about a technical domain, quite often I will give non-technical reasons why I believe the proposed solution is worth to consider. I assume that many of the topics I will touch in this blog can also help API designers in other domains, and I would love to receive your feedback on that.","title":"Scope"},{"location":"about/#be-warned","text":"Not all of the concepts described in these articles did find their way into the API I've been working on and by that had a chance to proof their value in the field. So better don't count on everything being perfect. If you find room for improvement, or confusing (hopefully not contradicting) aspects, please drop a comment on the post if( window.location.pathname==\"/about/\" ){ var t=\"org\", d=\"meshcaline\", n=\"meshcalero\"; var m=n+\"@\"+d+\".\"+t; document.write('or send me an email to <a href=\"mailto:'+m+'\">'+m+'</a>');}","title":"Be warned"},{"location":"about/#about-me","text":"If you're interested to know more about me you might want to visit my Linkedin Profile as a starting point.","title":"About me"},{"location":"basics/","text":"Basic meshcaline meshcaline propagates a hypertext based API design paradigm for HTTP(S) based web service API. It is intended to suit service products that want to expose their offering primarily through an API (\"The API is the product\"). As such they need to expose a simple to understand and easy to use API (otherwise it is unlikely that the product will become successful) that allows for ongoing enhancements. The basic idea of the meshcaline API design is quite simple: A meshcaline API represents the business processes supported by the service offering. Most non-trivial business processes consist of multiple steps. At each step either the developer or the end user of the developer's app decides, based on information obtained in the previous step and additional information contributed by the app/user, where to go / what they need next. Entering such a business process as well as each subsequent step of the business process corresponds to an API request, with responses containing necessary information and hyperlinks to the next possible steps. In short: The meshcaline API design adopts the interaction principles of web sites to web service API. While web sites are designed for human beings, capable grasping a lot out of unstructured context information which then allows them to pick and choose from the available hypertext options, APIs must be designed for algorithms and the developers that have to implement those algorithms. And while it is quite easy to for human being to identify that a link they followed isn't appropriate for what they were looking for, developers (and their algorithms) definitively don't want to make API calls and analyze the response, just to find out that this is not what they need. Therefore one of the most important parts of the meshcaline API design is the exposure of meta information along with the hyperlinks that supports developers in the decision making which of the alternative subsequent requests they want to make or offer to their end users for selection. The combination of hyperlinks and their corresponding meta information is called hypertext controls. meshcaline implements the HATEOAS constraint, but provides quite different means to interact with the API than Roy Fielding's REST rules do enforce. By following the meshcaline API design paradigms, you might still realize that you end up with a product that is more RESTful than most of the other so-called REST API you find out there on the web. Your API should be on level-3 according to the Richardson Maturity Model . Content types and resource types meshcaline APIs can use any regular content type that allows embedding hypertext controls, including popular ones like json, xml, html or their binary counterparts. A meshcaline API may support various content types simultaneously and allow clients to use HTTP content negotiation mechanisms to indicate the one they prefer. Contrary to plain REST meshcaline introduces explicitly a concept for resource types used in hypertext controls embedded in API responses. Every hypertext control identifies the type of resource the URI is pointing to. Resource types use identifiers specified in the API documentation. It is important to emphasize that resource types are independent from content types. A resource type specifies the content elements required to describe the concept a resource represents. Those elements may then be serialized in various formats, which are specified by the content type. The documentation of an API usually specifies the set of supported content types for a given resource type. Resource types are also used to specify, where necessary, the type of resource a client has to send to the API with a request message. Equally to the resource types for API responses, meshcaline doesn't enforce any specific content type used to serialize a resource type. Only for resource types describing request parameter for GET methods a standard binding to URI query string parameters has to be applied. meshcaline 's resource types are primarily used to describe the hypertext resources within a given API. For links to other web resource types (e.g. jpeg-files, web-sites), esp. those outside the territory of the API, it is usually simpler and equally appropriate to provide the resource's URI as value of a specific content attribute. Nevertheless it is perfectly valid to use hypertext controls for those links too and if an API implementation decides to do this, it is only required to specify resource types of those links too. It is recommended to use a regular media-range value as specified for the HTTP accept header as resource type identifiers for those links. Three resource types are predefined: #none : The resource doesn't have any representation #any : The resource may return anything; #implied : The resource type is equal to the type specification of the hypertext control's context (the element that contains the control). The type #implied helps avoiding inflationary definitions of resource types. While an API documentation may only describe the mayor response types, such a type introduces an anonymous resource type that is defined as equally to the specification of the sub-element within the resource type they are embedded with. For many structured content types used in API, there also exists some schema language that allows specifying the syntax and semantics of a given entity of that content type. If a meshcaline API provides such schema for their resource types within the content types it supports, then the resource type identifier should allow resolving the corresponding schema element. In most examples throughout this site JSON will be used as content type, simply because its the least noisy and easiest to read for human beings amongst the popular representations. This should not imply any restriction on the applicability of meshcaline to other content types. Hypertext controls The hypertext controls in meshcaline are used to link between individual API resources representing the available steps in a given business process. They provide client applications three things: Describe the relationship between a given API resource and the API resource the control links to Ensure that the resource type returned from that linked resource is a type the application is able / willing to process Verify that the application knows everything needed to construct the request to the linked resource properly The hypertext controls and the API's resource type specification gives client application developers everything necessary to know why, if and how to successfully follow a specific link. Contrary to the hypertext controls in HTML (e.g. forms) the meshcaline hypertext controls in general don't aim to support generic UI components, although a specific API might use them for that purpose too. The hypertext controls provide various information elements to the application A URI linking to the other API resource ( href ) The resource type of the target response ( type ) The HTTP method to be used for that hyperlink ( method ) The resource type(s) specifying the data expected from the resource ( accept ). The authentication schema(s) supported for the target resource ( auth ) A link-relation type, describing the type of relationship between the resource that contains the control and the resource it links to. The link-relation type is not a specific attribute of a hypertext control. Instead the name of the data element that represents the hypertext control shall be the type of relationship. Is is recommended to use nouns for hypermedia controls of related resources (e.g. author ) and verbs for those that represent client/user activities (e.g. delete ). meshcaline encourages the usage of standard link-relation types where appropriate ( next , previous , self ...). In addition to the above elements, a hypertext control may contain arbitrary attributes which the API designer deems to be relevant for the application developer in the context of the link. meshcaline Example 1: Let's start with a simple JSON representation of a resource type #song that also contains a link to a resource representing the interpreter of that song { \"title\": \"London Calling\", \"interpreter\": { \"name\": \"The Clash\", \"href\": \"https:/....\", \"type\": \"#artist\", \"accept\": \"#none\", \"method\": \"GET\" \"auth\": \"BASIC\", ... }, ... } In this example the JSON attribute \"interpreter\" is a hypertext control: interpreter specifies the link-relation-type between the song and the linked resource href contains the URI linking to the target resource type specifies that the target resource is of type #artist method specifies that the application should use HTTP GET to access the resource auth indicates that the resource will require BASIC authentication The attribute name is a regular data attribute within the hypertext control. Allowing arbitrary additional attributes in a hypertext control is an important part of meshcaline . Claiming to model the individual decision points in a business process, a control has to provide all information that allows to make that decision. In some cases the existence of the link and its relation type might be sufficient. But usually, and especially when the end user is supposed to make the decision, the application must present some UI element to the end-user that indicates based on the additional attributes in the hypertext control, what this decision is about. Those additional attributes may describe either the resource the control is linking to or give additional information related to the relationship between the two linked resources or both. Example 2: Let the representation of the #artist resource type, when representing a band, also list the individual members of the band { \"name\" : \"The Clash\", \"members\" : [ { \"name\": \"Joe Strummer\", \"instruments\": [\"lead vocals\", \"guitar\", \"harmonica\", \"keyboards\"] \"href\": \"https:/....\", \"type\": \"#artist\", \"accept\": \"#none\", \"method\": \"GET\" \"auth\": \"BASIC\", ... },... ], ... } In that context, the hypertext control to the resource type #artist not only contains the name of the artist, but also the instruments the artist played in that band. This information is not an attribute of the linked artist, but specific to the relationship between this band and the artist, as the artist might have played other instruments in other bands. meshcaline 's hypertext controls don't come with any special meta objects. They are nothing but standardized names for attributes of regular content elements. While other hypertext schema introduce special meta-objects that allow client applications to \"discover\" hyperlinks in a representation, meshcaline doesn't. Initially we thought this might be a nice concept, but during user testing we had to learn that our users anyway don't want to search for hyperlinks. They want to know at coding time, which content element represents a link they might have to follow. As a consequence meshcaline forces the documentation of a resource type to indicate which elements are hypertext controls. Nevertheless meshcaline allows extending an API over time as the capabilities of the API evolve in a backwards compatible way. Example 3: Assume in your initial API version, the \"#song\" resource did not yet have a dedicated resource for the interpreter, but only some attributes. { \"title\": \"London Calling\", \"interpreter\" : { \"name\": \"The Clash\", ... }, ... } In a subsequent versions you could still extend \"interpreter\" to become a hypertext control as shown in Example 1 in a backwards compatible way. You might have already realized that backwards compatibility in this example is only given under the assumption that the client application does ignore unknown elements in a response. A dedicated article on compatibility will give a more thorough introduction on rules around extensibility and backward compatibility with meshcaline Processing rules In addition to the fundamental hypertext control elements, a set of processing rules ensures that not every hypertext control must contain all attributes, and by that reduce the noise and consumed bandwidth introduced by the hypertext controls: method : HTTP method defaults to GET auth : If no authentication schema is given, the same authentication schema as provided for the origin resource is appropriate In many cases API need just a single authentication mechanism (if at all), and then the auth parameter is never needed. But some API may require different types of authentication for different API resources (some public resources, some protected resources which require the client to authorize, some personalized resources which require authentication of the end-user of the client application). To achieve the goal that the hypermedia control provides clients means to successfully construct the next request, the hypertext control indicates any required switch of authorization. accept : For HTTP methods without request body (most important GET ) accept defaults to #none . For HTTP methods that contain a request body, it defaults to #implied . Example 4: Assuming the origin resource was already fetched with BASIC authentication rules 1 to 3 would allow a much more readable representation for the hypertext control of example 1 { \"title\" : \"London Calling\", \"interpreter\" : { \"name\" : \"The Clash\", \"href\" : \"https:/....\", \"type\" : \"#artist\", ... }, ... } type : If no resource type is specified #implied is implied. Example 5: Assume your API allows you to post some star rating for each song { \"title\": \"London Calling\", \"interpreter\": { ... }, \"rating\": { \"average\": \"3.8\", \"count\": \"17\", \"create\": { \"href\": \"https://...\", \"method\": \"POST\", \"accept\": \"#ratingValue\", ... } }... } While the request body is specified with resource type #ratingValue , the control doesn't explicitly specify a target response type. The response will therefore have the identical structure as the context the create link is embedded in (the rating element): { \"average\" : \"3.9\", \"count\" : \"18\", \"create\" : { \"href\":\"https://\", \"method\":\"POST\", \"accept\":\"#ratingValue\", ... } } For specific link-relation types a meshcaline API may represent a whole hypertext control with just a URI string and define specific values for the hypertext control attributes of that type (except for the href attribute) that are valid throughout the whole API. This rule should only get applied to standard link relation types , like self , next , etc and will in most practical cases be limited to GET methods. In those cases the simplified hypertext control representation is more readable and fully sufficient. Example 6: A resource type #album shall contain a linked list of cover-art jpegs { \"title\" : \"London Calling\", \"coverart\" : { \"src\" : \"http://\", \"page\" : \"front\", \"next\": \"https://,... }, ... } This example next is a link relationship type that uses the simplified representation. The coverart element instead represents the URI to the actual cover binary as data-element src , not as hypertext control. Those rules do not necessarily simplify the processing of the hypermedia controls -- they even add some complexity on the client's processing logic. Also the reduced bandwidth will only be marginal -- as soon as you in start working with hypermedia you should anyway use gzip compression as transfer encoding and this would eat up most of the redundancy too. The primary reason for those rules is noise reduction -- not for technical reasons but for clarity: While API output is primarily going to be consumed by algorithms, the prerequisite for any output to get consumed by an algorithm is that a developer who writes those algorithms understands the API. The less avoidable information you have in your representation, the easier it becomes for a developer to understand the essence of you API.","title":"Basic meshcaline"},{"location":"basics/#basic-meshcaline","text":"meshcaline propagates a hypertext based API design paradigm for HTTP(S) based web service API. It is intended to suit service products that want to expose their offering primarily through an API (\"The API is the product\"). As such they need to expose a simple to understand and easy to use API (otherwise it is unlikely that the product will become successful) that allows for ongoing enhancements. The basic idea of the meshcaline API design is quite simple: A meshcaline API represents the business processes supported by the service offering. Most non-trivial business processes consist of multiple steps. At each step either the developer or the end user of the developer's app decides, based on information obtained in the previous step and additional information contributed by the app/user, where to go / what they need next. Entering such a business process as well as each subsequent step of the business process corresponds to an API request, with responses containing necessary information and hyperlinks to the next possible steps. In short: The meshcaline API design adopts the interaction principles of web sites to web service API. While web sites are designed for human beings, capable grasping a lot out of unstructured context information which then allows them to pick and choose from the available hypertext options, APIs must be designed for algorithms and the developers that have to implement those algorithms. And while it is quite easy to for human being to identify that a link they followed isn't appropriate for what they were looking for, developers (and their algorithms) definitively don't want to make API calls and analyze the response, just to find out that this is not what they need. Therefore one of the most important parts of the meshcaline API design is the exposure of meta information along with the hyperlinks that supports developers in the decision making which of the alternative subsequent requests they want to make or offer to their end users for selection. The combination of hyperlinks and their corresponding meta information is called hypertext controls. meshcaline implements the HATEOAS constraint, but provides quite different means to interact with the API than Roy Fielding's REST rules do enforce. By following the meshcaline API design paradigms, you might still realize that you end up with a product that is more RESTful than most of the other so-called REST API you find out there on the web. Your API should be on level-3 according to the Richardson Maturity Model .","title":"Basic meshcaline"},{"location":"basics/#content-types-and-resource-types","text":"meshcaline APIs can use any regular content type that allows embedding hypertext controls, including popular ones like json, xml, html or their binary counterparts. A meshcaline API may support various content types simultaneously and allow clients to use HTTP content negotiation mechanisms to indicate the one they prefer. Contrary to plain REST meshcaline introduces explicitly a concept for resource types used in hypertext controls embedded in API responses. Every hypertext control identifies the type of resource the URI is pointing to. Resource types use identifiers specified in the API documentation. It is important to emphasize that resource types are independent from content types. A resource type specifies the content elements required to describe the concept a resource represents. Those elements may then be serialized in various formats, which are specified by the content type. The documentation of an API usually specifies the set of supported content types for a given resource type. Resource types are also used to specify, where necessary, the type of resource a client has to send to the API with a request message. Equally to the resource types for API responses, meshcaline doesn't enforce any specific content type used to serialize a resource type. Only for resource types describing request parameter for GET methods a standard binding to URI query string parameters has to be applied. meshcaline 's resource types are primarily used to describe the hypertext resources within a given API. For links to other web resource types (e.g. jpeg-files, web-sites), esp. those outside the territory of the API, it is usually simpler and equally appropriate to provide the resource's URI as value of a specific content attribute. Nevertheless it is perfectly valid to use hypertext controls for those links too and if an API implementation decides to do this, it is only required to specify resource types of those links too. It is recommended to use a regular media-range value as specified for the HTTP accept header as resource type identifiers for those links. Three resource types are predefined: #none : The resource doesn't have any representation #any : The resource may return anything; #implied : The resource type is equal to the type specification of the hypertext control's context (the element that contains the control). The type #implied helps avoiding inflationary definitions of resource types. While an API documentation may only describe the mayor response types, such a type introduces an anonymous resource type that is defined as equally to the specification of the sub-element within the resource type they are embedded with. For many structured content types used in API, there also exists some schema language that allows specifying the syntax and semantics of a given entity of that content type. If a meshcaline API provides such schema for their resource types within the content types it supports, then the resource type identifier should allow resolving the corresponding schema element. In most examples throughout this site JSON will be used as content type, simply because its the least noisy and easiest to read for human beings amongst the popular representations. This should not imply any restriction on the applicability of meshcaline to other content types.","title":"Content types and resource types"},{"location":"basics/#hypertext-controls","text":"The hypertext controls in meshcaline are used to link between individual API resources representing the available steps in a given business process. They provide client applications three things: Describe the relationship between a given API resource and the API resource the control links to Ensure that the resource type returned from that linked resource is a type the application is able / willing to process Verify that the application knows everything needed to construct the request to the linked resource properly The hypertext controls and the API's resource type specification gives client application developers everything necessary to know why, if and how to successfully follow a specific link. Contrary to the hypertext controls in HTML (e.g. forms) the meshcaline hypertext controls in general don't aim to support generic UI components, although a specific API might use them for that purpose too. The hypertext controls provide various information elements to the application A URI linking to the other API resource ( href ) The resource type of the target response ( type ) The HTTP method to be used for that hyperlink ( method ) The resource type(s) specifying the data expected from the resource ( accept ). The authentication schema(s) supported for the target resource ( auth ) A link-relation type, describing the type of relationship between the resource that contains the control and the resource it links to. The link-relation type is not a specific attribute of a hypertext control. Instead the name of the data element that represents the hypertext control shall be the type of relationship. Is is recommended to use nouns for hypermedia controls of related resources (e.g. author ) and verbs for those that represent client/user activities (e.g. delete ). meshcaline encourages the usage of standard link-relation types where appropriate ( next , previous , self ...). In addition to the above elements, a hypertext control may contain arbitrary attributes which the API designer deems to be relevant for the application developer in the context of the link. meshcaline","title":"Hypertext controls"},{"location":"basics/#example-1","text":"Let's start with a simple JSON representation of a resource type #song that also contains a link to a resource representing the interpreter of that song { \"title\": \"London Calling\", \"interpreter\": { \"name\": \"The Clash\", \"href\": \"https:/....\", \"type\": \"#artist\", \"accept\": \"#none\", \"method\": \"GET\" \"auth\": \"BASIC\", ... }, ... } In this example the JSON attribute \"interpreter\" is a hypertext control: interpreter specifies the link-relation-type between the song and the linked resource href contains the URI linking to the target resource type specifies that the target resource is of type #artist method specifies that the application should use HTTP GET to access the resource auth indicates that the resource will require BASIC authentication The attribute name is a regular data attribute within the hypertext control. Allowing arbitrary additional attributes in a hypertext control is an important part of meshcaline . Claiming to model the individual decision points in a business process, a control has to provide all information that allows to make that decision. In some cases the existence of the link and its relation type might be sufficient. But usually, and especially when the end user is supposed to make the decision, the application must present some UI element to the end-user that indicates based on the additional attributes in the hypertext control, what this decision is about. Those additional attributes may describe either the resource the control is linking to or give additional information related to the relationship between the two linked resources or both.","title":"Example 1:"},{"location":"basics/#example-2","text":"Let the representation of the #artist resource type, when representing a band, also list the individual members of the band { \"name\" : \"The Clash\", \"members\" : [ { \"name\": \"Joe Strummer\", \"instruments\": [\"lead vocals\", \"guitar\", \"harmonica\", \"keyboards\"] \"href\": \"https:/....\", \"type\": \"#artist\", \"accept\": \"#none\", \"method\": \"GET\" \"auth\": \"BASIC\", ... },... ], ... } In that context, the hypertext control to the resource type #artist not only contains the name of the artist, but also the instruments the artist played in that band. This information is not an attribute of the linked artist, but specific to the relationship between this band and the artist, as the artist might have played other instruments in other bands. meshcaline 's hypertext controls don't come with any special meta objects. They are nothing but standardized names for attributes of regular content elements. While other hypertext schema introduce special meta-objects that allow client applications to \"discover\" hyperlinks in a representation, meshcaline doesn't. Initially we thought this might be a nice concept, but during user testing we had to learn that our users anyway don't want to search for hyperlinks. They want to know at coding time, which content element represents a link they might have to follow. As a consequence meshcaline forces the documentation of a resource type to indicate which elements are hypertext controls. Nevertheless meshcaline allows extending an API over time as the capabilities of the API evolve in a backwards compatible way.","title":"Example 2:"},{"location":"basics/#example-3","text":"Assume in your initial API version, the \"#song\" resource did not yet have a dedicated resource for the interpreter, but only some attributes. { \"title\": \"London Calling\", \"interpreter\" : { \"name\": \"The Clash\", ... }, ... } In a subsequent versions you could still extend \"interpreter\" to become a hypertext control as shown in Example 1 in a backwards compatible way. You might have already realized that backwards compatibility in this example is only given under the assumption that the client application does ignore unknown elements in a response. A dedicated article on compatibility will give a more thorough introduction on rules around extensibility and backward compatibility with meshcaline","title":"Example 3:"},{"location":"basics/#processing-rules","text":"In addition to the fundamental hypertext control elements, a set of processing rules ensures that not every hypertext control must contain all attributes, and by that reduce the noise and consumed bandwidth introduced by the hypertext controls: method : HTTP method defaults to GET auth : If no authentication schema is given, the same authentication schema as provided for the origin resource is appropriate In many cases API need just a single authentication mechanism (if at all), and then the auth parameter is never needed. But some API may require different types of authentication for different API resources (some public resources, some protected resources which require the client to authorize, some personalized resources which require authentication of the end-user of the client application). To achieve the goal that the hypermedia control provides clients means to successfully construct the next request, the hypertext control indicates any required switch of authorization. accept : For HTTP methods without request body (most important GET ) accept defaults to #none . For HTTP methods that contain a request body, it defaults to #implied .","title":"Processing rules"},{"location":"basics/#example-4","text":"Assuming the origin resource was already fetched with BASIC authentication rules 1 to 3 would allow a much more readable representation for the hypertext control of example 1 { \"title\" : \"London Calling\", \"interpreter\" : { \"name\" : \"The Clash\", \"href\" : \"https:/....\", \"type\" : \"#artist\", ... }, ... } type : If no resource type is specified #implied is implied.","title":"Example 4:"},{"location":"basics/#example-5","text":"Assume your API allows you to post some star rating for each song { \"title\": \"London Calling\", \"interpreter\": { ... }, \"rating\": { \"average\": \"3.8\", \"count\": \"17\", \"create\": { \"href\": \"https://...\", \"method\": \"POST\", \"accept\": \"#ratingValue\", ... } }... } While the request body is specified with resource type #ratingValue , the control doesn't explicitly specify a target response type. The response will therefore have the identical structure as the context the create link is embedded in (the rating element): { \"average\" : \"3.9\", \"count\" : \"18\", \"create\" : { \"href\":\"https://\", \"method\":\"POST\", \"accept\":\"#ratingValue\", ... } } For specific link-relation types a meshcaline API may represent a whole hypertext control with just a URI string and define specific values for the hypertext control attributes of that type (except for the href attribute) that are valid throughout the whole API. This rule should only get applied to standard link relation types , like self , next , etc and will in most practical cases be limited to GET methods. In those cases the simplified hypertext control representation is more readable and fully sufficient.","title":"Example 5:"},{"location":"basics/#example-6","text":"A resource type #album shall contain a linked list of cover-art jpegs { \"title\" : \"London Calling\", \"coverart\" : { \"src\" : \"http://\", \"page\" : \"front\", \"next\": \"https://,... }, ... } This example next is a link relationship type that uses the simplified representation. The coverart element instead represents the URI to the actual cover binary as data-element src , not as hypertext control. Those rules do not necessarily simplify the processing of the hypermedia controls -- they even add some complexity on the client's processing logic. Also the reduced bandwidth will only be marginal -- as soon as you in start working with hypermedia you should anyway use gzip compression as transfer encoding and this would eat up most of the redundancy too. The primary reason for those rules is noise reduction -- not for technical reasons but for clarity: While API output is primarily going to be consumed by algorithms, the prerequisite for any output to get consumed by an algorithm is that a developer who writes those algorithms understands the API. The less avoidable information you have in your representation, the easier it becomes for a developer to understand the essence of you API.","title":"Example 6:"},{"location":"evolution/","text":"API Evolution & Compatibility Iterative methodologies are best practice for product development. Even among those who don't practice Agile methodologies, nobody would try to develop a non-trivial software product with a waterfall process. It is just a matter of fact that it is impossible to collect all requirements of a to be built product upfront and then correctly specify the solution for all those requirements that once implemented will meet the expectations of its users. This doesn't mean that you should not try to, but at the same time you have to remind yourself that this is just a plan and as an old military wisdom says: No plan of operations extends with any certainty beyond the first contact with the main hostile force -- Helmuth von Moltke, On Strategy (1890) better known in its shorter version: No plan survives first contact with the enemy The simple truth is that you can't be sure how good your product actually is until customers (try to) use it. Then you learn from their feedback and iterate. This is true for any kind of product and no different for an API product. When you assume your first version of your API will be perfect, be assured that you will be proven wrong. Even if you believe you know your (future) customer's needs inside out, they will soon hurt you by pointing to various aspects where your design and functionality doesn't fully meet their expectations. And once you have satisfied your first customers and they have integrated your API into their solution, the next wave of potential customers will surprise you with just another set of issues and gaps. As a consequence you must be prepared for iterative API evolution. By that you will incrementally add missing functionality and remove aspects that are of no use for your customers. Unfortunately that's where a fundamental difference in API based products makes your life very difficult. While (software) products that get shipped can still be used by their users after that product version has been replaced with a new one, when you serve your product via an API, then you have to make sure that you don't break existing clients when you release a better but different version of your API. In theory there is a simple mechanism to manage that problem: Treat you API like a shipped product: once shipped it doesn't get changed anymore. Unfortunately for most real-live API products this is impractical, esp. when you apply Agile methodologies and try to release early and frequently: Even when you don't plans to touch your previous versions any more, you will have to. Any kind of software service needs maintenance for at least two reasons: * You will have bugs in your software that need to get fixed. * Nobody builds software that is not based on other software components. And each of those components might release new versions and you will have important reasons to upgrade (e.g. performance improvements, security fixes, ..). Even when you're not applying continuous-delivery as part of your software development process and only release your updates at the end of your sprints, you'll easily end up with 10-20 additional versions of your API per year that you have to maintain. Very soon your development team will do nothing else but maintenance. Backward compatibility As a consequence of the above you have to accept the much more cumbersome approach: Backward compatibility . Unfortunately this approach restricts you in the amount of change you are allowed to apply to your API. The meshcaline design helps you minimizing the limitations for backwards compatibility. Hyperlinks rather than URI templates As already described in \"Why Hypertext API?\" meshcaline's hypertext based design allows you to change the addressing scheme of the individual resources of your API, including splitting up the API implementation into multiple service. While there are various mechanisms how you could split up the implementation of a growing API, they all come at least at the expense of the introduction of an additional component that does the traffic routing, which introduces an avoidable single-point-of-failure. With a meshcaline API you get a scalable solution for free as your hypertext controls would always point to the correct service. Extensible API contract By default meshcaline API are open for extension. Their API documentation (and when available their schemas) of their resource types allow you to add additional resources, add new resource types and modify existing resource types as long as those changes are still backwards compatible to previous versions of your API. Unfortunately backward compatibility is a tricky beast and you can break compatibility by different means: Syntactic changes With syntactic changes we refer to any type of change of the specification of the resource types of an API. Ideally the specification exists in form of some formal schema and you can easily verify that your current API's resource types also match the specification of your previous versions. Since resource types can be used for both retrieving data from your API (response types) and sending data to your API (request types), both, your service and your clients, will have to ignore any unexpected data element. You have to be careful what kind of syntax change you introduce, as depending on what the changed resource type is used for they may or may not break compatibility: Type of Change Request Type Response Type Adding an optional element ok ok Adding a conditional element error ok Adding a mandatory element error ok Removing an optional element ok warning Removing a conditional element ok error Removing a mandatory element ok error Extending the value range of an existing data element ok error Restricting the value range of an exiting data element error ok Widening the size constrains of collection elements ok error Narrowing the size constrains of collection elements error ok As you can see the only type of change you can safely apply to the specification of your resource types is adding optional elements. When a resource type is only used for responses you have a bit more freedom to change. Sometimes you can also take advantage of the allowed changes for request types, but in practice most API also respond in some form the data elements of request type and as a result write-only data types are quite rate. For resource types that get used for both request and responses, you might have to introduce specific variants. This is often anyway recommended because request types usually do not contain the hypermedia controls embedded in response types. Defining distinct request/response types would also allow you to use different constraints for the same elements for added data elements: While the response type guarantees that a new data element is always available (mandatory) in the request type it is only optional, so that old clients still work. It happens quite often that you realize that you have to replace a simple data element with a more complex data type. In that case it is best practice to keep the old definition as deprecated data element in your resource types and put the more granular new data type in an additional data element. Also here you might be required to introduce separate type definitions for request and response types. Semantic changes Semantic changes could get applied to a resource type as a whole as well as to the semantic of the individual elements of a resource type. Examples for classes of semantic changes include: Modification : An API for holiday homes has a price element that contains the price of a holiday apartment per week . If a future version would decide to represent the price per day this modifies the semantic although the syntax of the resource type has not changed. Generalization : If a resource type #song was explicitly specified to represent the concept of a music stream then a future version could generalize it to audio stream and contain e.g. an audio book stream if all individual elements of the resource type would be identical. Specialization : Similar to generalization, specialization changes the semantic: When we realize that all clients of our service are only interested in Punk music, then we might kick out all other types of music from our system. Type of Change Request Type Response Type Modification error error Generalization ok warning Specialization error warning Semantic changes are more subtle than syntactic changes. Usually no client application will break directly, but the quality of your client's application will be impacted, as they know longer deliver the expected semantic. As a result the chance will most likely cause harm to your business processes. Due to that is recommended to avoid semantic changes unless you fully understand their implications and can deal with them. Non-functional changes The aspect of non-functional API changes is ofter forgotten, but they can also create backward incompatibilities for your clients. Let's look into some obviously harmful non-functional changes: Authentication : Assume you introduce some authentication mechanisms to your API that previously was accessible without. This obviously kills your existing clients Rate limits : Rate-limits or quotas are another common pitfall: While your are initially happy about everyone who is using our API, after it got popular you have to manage the load. As a result you might start limiting the use of your service by introducing hard rate limits or degraded service quality. While both cases seem obvious, I've seen plenty of examples where they were forgotten in the initial design of an API and then later could not get introduced without breaking existing clients. Given that you build your API product to become successful and then those aspects will become relevant, you should never postpone those design aspects to future versions. But there are also more subtle non-functional changes that you have to take care of: Request latency : Good customer experience has to be responsive. If your API is supposed to enable the implementation of a business process, then the latency of your service has a direct impact on the customer experience and with that quality of your clients implementation Response size : meshcaline API are expected to be called directly from end-user devices. Those might not have connectivity with high bandwidth and as a result also the response size will directly impact the responsiveness of your client's application. Usually adding some data elements should not cause harm, but other types of extensions will. Examples for this include collection resources (without pagination) where the number of entries growth over time or introducing large data elements (e.g. embedding the mp3 as Data URI directly in your response) Processing rules The documentation of a meshcaline API usually doesn't only contain the specification of syntax and semantic of your resource types. In addition to that it may also contain processing rules. We have already seen some of them in the description of meshcaline's data model for hypertext controls . Those processing rules got introduced to maximize conciseness and readability of the API responses. Another important purpose of processing rules is forward compatibility: They instruct your clients how to deal with expected future extensions of your API. We have seen the most important processing rule of meshcaline API already in the section above: \"Ignore any data element that you don't expect!\" Another example for a processing rule was used in the Extensible business process flows example, where we asked clients to \"Show the title of all related resources when you know its resource type and guide your users to them by using the provided link!\" The latter rule is basically an extension of the \"Ignore any data element ...\" rule, but instead of individual data elements it applies to individual items of an untyped collection. Other variants of processing rules might be based on some polymorphic relationship between all possible items in some collection, that would allow clients to even provide some generic functionality for resource types that were unknown at the time where the client application has been built. When designed carefully such rules not only allow you extend the feature set of your API incrementally. They may even allow you to automatically extend your client's applications too. Unfortunately processing rules have two major downsides: You ask your clients to invest in business logic that may or may not come and where they are not even sure they want/need it Neither you nor your clients have means to test if the processing rules have been implemented correctly. The first downside is a product question that you have to address first. If your promise of future extension is not strong enough for your clients to do the extra investment (and your processing rules are hopefully easy to implement), then you better don't introduce them. For the second downside meshcaline suggests a technical solution: Extend your API implementation with a fast-forward mode. When active you introduce random API extensions in your response types that match your processing rules. You might activate the fast-forward mode by default in your sandbox environment where your clients test their applications as part of their software development processes, and you might also allow your clients to activate the fast-forward model on your production service (e.g. by introducing a special header to each API request), to enable manual tests on production too. Think ahead! Backward compatibility is the mechanism that allows you to iteratively develop your product without maintaining additional versions with every release. As we have seen it unfortunately limits your freedom to change substantially. As a consequence you have to be prepared that there will be a time where you have to introduce a new, backward incompatible ( major ) version. This is not doomsday, it is a regular part of the lifecycle of your API product that you have to manage. Instead of fighting windmills by trying to avoid new major versions, the goal of your design efforts should be to introduce them so infrequently that it is economically feasible to maintain multiple major versions at the same time. How to exactly calculate this heavily depends on your business. As a rule of thumb I would recommend to start with a best case assumption that you have to maintain a major release for at least 2 additional years after the next major release has been launched: Not only will your clients have to find budget and time to invest into the migration. Given that meshcaline API are intended to be used directly from client applications, including binary applications installed on the end-users device, your clients will often not be in full control to upgrade all older versions of their applications either. In change-averse industries substantially longer maintenance periods are quite common, but in those cases your sales team hopefully negotiated a commercial agreement that compensates you for the additional maintenance costs. Better don't assume that the maintenance period you have written in your API contract will give you a guarantee that you can decommission old major versions at their end. As long as you still have substantial traffic on those versions, decommissioning will make your customers quite unhappy and with that cause harm to your business. In practice you will have to maintain older versions substantially longer (Microsoft maintained Windows XP >7 years longer than originally announced). The contract only guarantees you that nobody can sue you, in case have more important business reasons to discontinue the support. When you have to prepare yourself for maintaining old major versions for a long time, then one of the goals of your API design activities should be to maximize the duration between two major versions. The only way to do this is to think ahead: Make assumptions about possible extensions / changes that you see upcoming in the next 1-2 years and design how you could introduce those features without breaking compatibility for the version you are currently working on. Whenever I mention something like that, there is someone in the room who says: \"Isn't this violating the Agile principle of Do the simplest thing that might possibly work ?\". I believe this is one of the biggest misconceptions related to Agile practices. Agile is not about not thinking ahead, it is about not building what hasn't proven that it is actually needed. I have seen quite often situations where the implementation of a product had to start from scratch, because the initial version could not scale with the load. Any yes, the initial implementation that was only used in some A/B test did not have to be ready for scale. But this doesn't imply that you should ignore the future requirement for scalability when designing the initial version, esp. when it can be expected that this requirement will come once the initial version is successful. You don't have to implement the initial version already in a fully scalable way (this would not be Agile, as your product has not yet proven that it is successful), but you should think about how you can evolve it to a scalable version. And only when you then come to the conclusion that it economically doesn't make sense to already prepare for scalability, then you make (and communicate) a conscious business decision that you go into the A/B test with a to-be-thrown away prototype rather than with a first version of your future product. You don't have to design all the details of the possible extensions you could dream of. But you need to develop a solid understanding of how you might possibly integrate such extensions in a backward compatible way, once one of the expected requirements materializes. Should you realize that preparing the design of your next version for future extensions would make the next version harder to understand/integrate (e.g. by introducing complex processing rules for your clients), then you make the conscious business decision that it is not worth risking the success of the next version for a not yet proven extension and with that accept the possible consequence of introducing a new major version should the need for the extension materialize.","title":"API Evolution & Compatibility"},{"location":"evolution/#api-evolution-compatibility","text":"Iterative methodologies are best practice for product development. Even among those who don't practice Agile methodologies, nobody would try to develop a non-trivial software product with a waterfall process. It is just a matter of fact that it is impossible to collect all requirements of a to be built product upfront and then correctly specify the solution for all those requirements that once implemented will meet the expectations of its users. This doesn't mean that you should not try to, but at the same time you have to remind yourself that this is just a plan and as an old military wisdom says: No plan of operations extends with any certainty beyond the first contact with the main hostile force -- Helmuth von Moltke, On Strategy (1890) better known in its shorter version: No plan survives first contact with the enemy The simple truth is that you can't be sure how good your product actually is until customers (try to) use it. Then you learn from their feedback and iterate. This is true for any kind of product and no different for an API product. When you assume your first version of your API will be perfect, be assured that you will be proven wrong. Even if you believe you know your (future) customer's needs inside out, they will soon hurt you by pointing to various aspects where your design and functionality doesn't fully meet their expectations. And once you have satisfied your first customers and they have integrated your API into their solution, the next wave of potential customers will surprise you with just another set of issues and gaps. As a consequence you must be prepared for iterative API evolution. By that you will incrementally add missing functionality and remove aspects that are of no use for your customers. Unfortunately that's where a fundamental difference in API based products makes your life very difficult. While (software) products that get shipped can still be used by their users after that product version has been replaced with a new one, when you serve your product via an API, then you have to make sure that you don't break existing clients when you release a better but different version of your API. In theory there is a simple mechanism to manage that problem: Treat you API like a shipped product: once shipped it doesn't get changed anymore. Unfortunately for most real-live API products this is impractical, esp. when you apply Agile methodologies and try to release early and frequently: Even when you don't plans to touch your previous versions any more, you will have to. Any kind of software service needs maintenance for at least two reasons: * You will have bugs in your software that need to get fixed. * Nobody builds software that is not based on other software components. And each of those components might release new versions and you will have important reasons to upgrade (e.g. performance improvements, security fixes, ..). Even when you're not applying continuous-delivery as part of your software development process and only release your updates at the end of your sprints, you'll easily end up with 10-20 additional versions of your API per year that you have to maintain. Very soon your development team will do nothing else but maintenance.","title":"API Evolution &amp; Compatibility"},{"location":"evolution/#backward-compatibility","text":"As a consequence of the above you have to accept the much more cumbersome approach: Backward compatibility . Unfortunately this approach restricts you in the amount of change you are allowed to apply to your API. The meshcaline design helps you minimizing the limitations for backwards compatibility.","title":"Backward compatibility"},{"location":"evolution/#hyperlinks-rather-than-uri-templates","text":"As already described in \"Why Hypertext API?\" meshcaline's hypertext based design allows you to change the addressing scheme of the individual resources of your API, including splitting up the API implementation into multiple service. While there are various mechanisms how you could split up the implementation of a growing API, they all come at least at the expense of the introduction of an additional component that does the traffic routing, which introduces an avoidable single-point-of-failure. With a meshcaline API you get a scalable solution for free as your hypertext controls would always point to the correct service.","title":"Hyperlinks rather than URI templates"},{"location":"evolution/#extensible-api-contract","text":"By default meshcaline API are open for extension. Their API documentation (and when available their schemas) of their resource types allow you to add additional resources, add new resource types and modify existing resource types as long as those changes are still backwards compatible to previous versions of your API. Unfortunately backward compatibility is a tricky beast and you can break compatibility by different means:","title":"Extensible API contract"},{"location":"evolution/#syntactic-changes","text":"With syntactic changes we refer to any type of change of the specification of the resource types of an API. Ideally the specification exists in form of some formal schema and you can easily verify that your current API's resource types also match the specification of your previous versions. Since resource types can be used for both retrieving data from your API (response types) and sending data to your API (request types), both, your service and your clients, will have to ignore any unexpected data element. You have to be careful what kind of syntax change you introduce, as depending on what the changed resource type is used for they may or may not break compatibility: Type of Change Request Type Response Type Adding an optional element ok ok Adding a conditional element error ok Adding a mandatory element error ok Removing an optional element ok warning Removing a conditional element ok error Removing a mandatory element ok error Extending the value range of an existing data element ok error Restricting the value range of an exiting data element error ok Widening the size constrains of collection elements ok error Narrowing the size constrains of collection elements error ok As you can see the only type of change you can safely apply to the specification of your resource types is adding optional elements. When a resource type is only used for responses you have a bit more freedom to change. Sometimes you can also take advantage of the allowed changes for request types, but in practice most API also respond in some form the data elements of request type and as a result write-only data types are quite rate. For resource types that get used for both request and responses, you might have to introduce specific variants. This is often anyway recommended because request types usually do not contain the hypermedia controls embedded in response types. Defining distinct request/response types would also allow you to use different constraints for the same elements for added data elements: While the response type guarantees that a new data element is always available (mandatory) in the request type it is only optional, so that old clients still work. It happens quite often that you realize that you have to replace a simple data element with a more complex data type. In that case it is best practice to keep the old definition as deprecated data element in your resource types and put the more granular new data type in an additional data element. Also here you might be required to introduce separate type definitions for request and response types.","title":"Syntactic changes"},{"location":"evolution/#semantic-changes","text":"Semantic changes could get applied to a resource type as a whole as well as to the semantic of the individual elements of a resource type. Examples for classes of semantic changes include: Modification : An API for holiday homes has a price element that contains the price of a holiday apartment per week . If a future version would decide to represent the price per day this modifies the semantic although the syntax of the resource type has not changed. Generalization : If a resource type #song was explicitly specified to represent the concept of a music stream then a future version could generalize it to audio stream and contain e.g. an audio book stream if all individual elements of the resource type would be identical. Specialization : Similar to generalization, specialization changes the semantic: When we realize that all clients of our service are only interested in Punk music, then we might kick out all other types of music from our system. Type of Change Request Type Response Type Modification error error Generalization ok warning Specialization error warning Semantic changes are more subtle than syntactic changes. Usually no client application will break directly, but the quality of your client's application will be impacted, as they know longer deliver the expected semantic. As a result the chance will most likely cause harm to your business processes. Due to that is recommended to avoid semantic changes unless you fully understand their implications and can deal with them.","title":"Semantic changes"},{"location":"evolution/#non-functional-changes","text":"The aspect of non-functional API changes is ofter forgotten, but they can also create backward incompatibilities for your clients. Let's look into some obviously harmful non-functional changes: Authentication : Assume you introduce some authentication mechanisms to your API that previously was accessible without. This obviously kills your existing clients Rate limits : Rate-limits or quotas are another common pitfall: While your are initially happy about everyone who is using our API, after it got popular you have to manage the load. As a result you might start limiting the use of your service by introducing hard rate limits or degraded service quality. While both cases seem obvious, I've seen plenty of examples where they were forgotten in the initial design of an API and then later could not get introduced without breaking existing clients. Given that you build your API product to become successful and then those aspects will become relevant, you should never postpone those design aspects to future versions. But there are also more subtle non-functional changes that you have to take care of: Request latency : Good customer experience has to be responsive. If your API is supposed to enable the implementation of a business process, then the latency of your service has a direct impact on the customer experience and with that quality of your clients implementation Response size : meshcaline API are expected to be called directly from end-user devices. Those might not have connectivity with high bandwidth and as a result also the response size will directly impact the responsiveness of your client's application. Usually adding some data elements should not cause harm, but other types of extensions will. Examples for this include collection resources (without pagination) where the number of entries growth over time or introducing large data elements (e.g. embedding the mp3 as Data URI directly in your response)","title":"Non-functional changes"},{"location":"evolution/#processing-rules","text":"The documentation of a meshcaline API usually doesn't only contain the specification of syntax and semantic of your resource types. In addition to that it may also contain processing rules. We have already seen some of them in the description of meshcaline's data model for hypertext controls . Those processing rules got introduced to maximize conciseness and readability of the API responses. Another important purpose of processing rules is forward compatibility: They instruct your clients how to deal with expected future extensions of your API. We have seen the most important processing rule of meshcaline API already in the section above: \"Ignore any data element that you don't expect!\" Another example for a processing rule was used in the Extensible business process flows example, where we asked clients to \"Show the title of all related resources when you know its resource type and guide your users to them by using the provided link!\" The latter rule is basically an extension of the \"Ignore any data element ...\" rule, but instead of individual data elements it applies to individual items of an untyped collection. Other variants of processing rules might be based on some polymorphic relationship between all possible items in some collection, that would allow clients to even provide some generic functionality for resource types that were unknown at the time where the client application has been built. When designed carefully such rules not only allow you extend the feature set of your API incrementally. They may even allow you to automatically extend your client's applications too. Unfortunately processing rules have two major downsides: You ask your clients to invest in business logic that may or may not come and where they are not even sure they want/need it Neither you nor your clients have means to test if the processing rules have been implemented correctly. The first downside is a product question that you have to address first. If your promise of future extension is not strong enough for your clients to do the extra investment (and your processing rules are hopefully easy to implement), then you better don't introduce them. For the second downside meshcaline suggests a technical solution: Extend your API implementation with a fast-forward mode. When active you introduce random API extensions in your response types that match your processing rules. You might activate the fast-forward mode by default in your sandbox environment where your clients test their applications as part of their software development processes, and you might also allow your clients to activate the fast-forward model on your production service (e.g. by introducing a special header to each API request), to enable manual tests on production too.","title":"Processing rules"},{"location":"evolution/#think-ahead","text":"Backward compatibility is the mechanism that allows you to iteratively develop your product without maintaining additional versions with every release. As we have seen it unfortunately limits your freedom to change substantially. As a consequence you have to be prepared that there will be a time where you have to introduce a new, backward incompatible ( major ) version. This is not doomsday, it is a regular part of the lifecycle of your API product that you have to manage. Instead of fighting windmills by trying to avoid new major versions, the goal of your design efforts should be to introduce them so infrequently that it is economically feasible to maintain multiple major versions at the same time. How to exactly calculate this heavily depends on your business. As a rule of thumb I would recommend to start with a best case assumption that you have to maintain a major release for at least 2 additional years after the next major release has been launched: Not only will your clients have to find budget and time to invest into the migration. Given that meshcaline API are intended to be used directly from client applications, including binary applications installed on the end-users device, your clients will often not be in full control to upgrade all older versions of their applications either. In change-averse industries substantially longer maintenance periods are quite common, but in those cases your sales team hopefully negotiated a commercial agreement that compensates you for the additional maintenance costs. Better don't assume that the maintenance period you have written in your API contract will give you a guarantee that you can decommission old major versions at their end. As long as you still have substantial traffic on those versions, decommissioning will make your customers quite unhappy and with that cause harm to your business. In practice you will have to maintain older versions substantially longer (Microsoft maintained Windows XP >7 years longer than originally announced). The contract only guarantees you that nobody can sue you, in case have more important business reasons to discontinue the support. When you have to prepare yourself for maintaining old major versions for a long time, then one of the goals of your API design activities should be to maximize the duration between two major versions. The only way to do this is to think ahead: Make assumptions about possible extensions / changes that you see upcoming in the next 1-2 years and design how you could introduce those features without breaking compatibility for the version you are currently working on. Whenever I mention something like that, there is someone in the room who says: \"Isn't this violating the Agile principle of Do the simplest thing that might possibly work ?\". I believe this is one of the biggest misconceptions related to Agile practices. Agile is not about not thinking ahead, it is about not building what hasn't proven that it is actually needed. I have seen quite often situations where the implementation of a product had to start from scratch, because the initial version could not scale with the load. Any yes, the initial implementation that was only used in some A/B test did not have to be ready for scale. But this doesn't imply that you should ignore the future requirement for scalability when designing the initial version, esp. when it can be expected that this requirement will come once the initial version is successful. You don't have to implement the initial version already in a fully scalable way (this would not be Agile, as your product has not yet proven that it is successful), but you should think about how you can evolve it to a scalable version. And only when you then come to the conclusion that it economically doesn't make sense to already prepare for scalability, then you make (and communicate) a conscious business decision that you go into the A/B test with a to-be-thrown away prototype rather than with a first version of your future product. You don't have to design all the details of the possible extensions you could dream of. But you need to develop a solid understanding of how you might possibly integrate such extensions in a backward compatible way, once one of the expected requirements materializes. Should you realize that preparing the design of your next version for future extensions would make the next version harder to understand/integrate (e.g. by introducing complex processing rules for your clients), then you make the conscious business decision that it is not worth risking the success of the next version for a not yet proven extension and with that accept the possible consequence of introducing a new major version should the need for the extension materialize.","title":"Think ahead!"},{"location":"filters/","text":"Consumer Driven Response Types A meshcaline API supports Consumer Driven Resource Types , an API feature that allows you to manage backward incompatible changes in a controlled way, and by that help reducing the negative implications of those. Let's first look into two common problems where this becomes beneficial: Experimental features As we have seen in API Evolution & Compatibility we have to carefully plan any change to our API to make sure that the change is (a) backward compatible to previous API version and (b) enables the introduction of additional features in the future. But this is not aways possible. Daily practice shows us, that regardless how hard we try, it usually requires some iterations until a problem is well enough understood and a solution's API design is mature and sustainable. This seems to be a unresolvable conflict to backward compatibility. How can you experiment with your API design if you always have to make sure that you neither break the past nor the future? The purpose of an experiment is to learn from mistakes and then iterate until you have found an appropriate e solution. One approach to overcome this dilemma is to run those experiments in an isolated environment. Only a set of pilot clients where you can manage the introduction of backward incompatible changes in the various iterations of the experiment is allowed to use this environment. Unfortunately this can become quite expensive, as you have to operate and maintain those dedicated environments. And in case you have more of those experiments ongoing at the same time and with different clients, then you might even need to have dedicated environments for specific combinations of experiments. Another popular approach that tries to overcome that problem is \"information hiding\". By not mentioning the experimental features and resources in the API documentation, one assumes that only those clients participating in the experiment will know about the new capabilities (as only they get access to the corresponding documentation), and with that it is safe to run those experiments on production systems. Unfortunately this is a wrong assumption: Developers have a natural distrust to documentation and trust more what they see. As a result you will see that developers outside of the group of experiment participants will discover the new feature when they play with your API e.g. with Postman . And once they like what they see, they will start using it, regarless of what is written in your documentation. For exactly that reason Google's API guidelines don't allow their developers to ever change any behaviour or semantics of production API, even when such behavior is not explicitly supported or documented . Feature deprecation For another type of problem let's pick a typical example: When you launched the API for concert ticket business you were a small local company, focusing on the US market and due to that amount in your price attrbiute in your API was implicitly in US$. As your business grows you want to expand into other regions and also sell tickets cross border with currency conversion. To avoid that you have to introduce a new major API version for that business expansion, you decide to introduce two additional data elements ticket_price and customer_price next to the existing price , both containing two attributes amount and currency . The old price attribute gets deprecated, but for your legacy clients who only request tickets for the US, the same API still does the job. After some time, you learn from your clients that they now use ticket_price and customer_price too, in order to participate in the new international business. As every deprecated feature adds a smell to your API (and the risk that also new clients would still use it) you would like to get rid of the old price attribute. Unfortunately deleting a mandatory element is a backward incompatible change. And you don't have big enough business case to introduce a new major version (neither for yourself, nor for your clients to migrate), as the smell (like many other forms of technical debt) doesn't directly hurt anyone. Telling me what you need! The meshcaline design allows you to address both problems with a single solution. It is derived from Consumer Driven Contracts , where your clients tell you what behaviour they expect from you by providing you test cases that you embed into your API's build pipeline. Whenever a new version breaks an existing consumer contract your build breaks. Then you know that you have to either change your new version or talk with your client to resolve the problem before the next release. While this is a powerful tool when you have a close relationship with a handful of customers, it doesn't fit for public API with a potential large number of clients, some of which you might not even know. Still, the idea of knowing what exactly your customers expect is compelling. Instead, of sending you tests, with meshcaline's Consumer Driven Response Types your clients use a query language on the data elements of your response types to tell you exactly which data elements they need. And then your API only responds exactly those elements. Instead of inventing a new query language, meshcaline's Consumer Driven Resource Types reuse the well defined query language of Facebook's GraphQL . What do you get from that? With Consumer Driven Resource Types you can rely on the \"information hiding\" approach for experimental features, as clients not participating in the experiment would not be able to discover the experminatal feature by chance when playing with your API. And you can also securely remove deprecated features when you see that they are no longer in use. While this is in theory still a backward incompatible change, as long as it doesn't break anything, who cares? But there are more advantages that come with Consumer Driven Resource Types : By only responding what has been explicitly requested your clients can make best use of the available bandwidth to your API and you have eliminated the risk of non-functional backwards incompatibility introduced by additional large data elements. Rather than ignoring items of untyped collections they are not aware of, your clients tell you which types of items the can/want to handle. This helps avoiding situations where clients have to iterate through multiple pages of collections before they get the elements they can use. You can measure the popularity of each individual data element, which gives you a significant better understanding about your clients than you would have if you would only track the access on the level of resources.","title":"Consumer Driven Response Types"},{"location":"filters/#consumer-driven-response-types","text":"A meshcaline API supports Consumer Driven Resource Types , an API feature that allows you to manage backward incompatible changes in a controlled way, and by that help reducing the negative implications of those. Let's first look into two common problems where this becomes beneficial:","title":"Consumer Driven Response Types"},{"location":"filters/#experimental-features","text":"As we have seen in API Evolution & Compatibility we have to carefully plan any change to our API to make sure that the change is (a) backward compatible to previous API version and (b) enables the introduction of additional features in the future. But this is not aways possible. Daily practice shows us, that regardless how hard we try, it usually requires some iterations until a problem is well enough understood and a solution's API design is mature and sustainable. This seems to be a unresolvable conflict to backward compatibility. How can you experiment with your API design if you always have to make sure that you neither break the past nor the future? The purpose of an experiment is to learn from mistakes and then iterate until you have found an appropriate e solution. One approach to overcome this dilemma is to run those experiments in an isolated environment. Only a set of pilot clients where you can manage the introduction of backward incompatible changes in the various iterations of the experiment is allowed to use this environment. Unfortunately this can become quite expensive, as you have to operate and maintain those dedicated environments. And in case you have more of those experiments ongoing at the same time and with different clients, then you might even need to have dedicated environments for specific combinations of experiments. Another popular approach that tries to overcome that problem is \"information hiding\". By not mentioning the experimental features and resources in the API documentation, one assumes that only those clients participating in the experiment will know about the new capabilities (as only they get access to the corresponding documentation), and with that it is safe to run those experiments on production systems. Unfortunately this is a wrong assumption: Developers have a natural distrust to documentation and trust more what they see. As a result you will see that developers outside of the group of experiment participants will discover the new feature when they play with your API e.g. with Postman . And once they like what they see, they will start using it, regarless of what is written in your documentation. For exactly that reason Google's API guidelines don't allow their developers to ever change any behaviour or semantics of production API, even when such behavior is not explicitly supported or documented .","title":"Experimental features"},{"location":"filters/#feature-deprecation","text":"For another type of problem let's pick a typical example: When you launched the API for concert ticket business you were a small local company, focusing on the US market and due to that amount in your price attrbiute in your API was implicitly in US$. As your business grows you want to expand into other regions and also sell tickets cross border with currency conversion. To avoid that you have to introduce a new major API version for that business expansion, you decide to introduce two additional data elements ticket_price and customer_price next to the existing price , both containing two attributes amount and currency . The old price attribute gets deprecated, but for your legacy clients who only request tickets for the US, the same API still does the job. After some time, you learn from your clients that they now use ticket_price and customer_price too, in order to participate in the new international business. As every deprecated feature adds a smell to your API (and the risk that also new clients would still use it) you would like to get rid of the old price attribute. Unfortunately deleting a mandatory element is a backward incompatible change. And you don't have big enough business case to introduce a new major version (neither for yourself, nor for your clients to migrate), as the smell (like many other forms of technical debt) doesn't directly hurt anyone.","title":"Feature deprecation"},{"location":"filters/#telling-me-what-you-need","text":"The meshcaline design allows you to address both problems with a single solution. It is derived from Consumer Driven Contracts , where your clients tell you what behaviour they expect from you by providing you test cases that you embed into your API's build pipeline. Whenever a new version breaks an existing consumer contract your build breaks. Then you know that you have to either change your new version or talk with your client to resolve the problem before the next release. While this is a powerful tool when you have a close relationship with a handful of customers, it doesn't fit for public API with a potential large number of clients, some of which you might not even know. Still, the idea of knowing what exactly your customers expect is compelling. Instead, of sending you tests, with meshcaline's Consumer Driven Response Types your clients use a query language on the data elements of your response types to tell you exactly which data elements they need. And then your API only responds exactly those elements. Instead of inventing a new query language, meshcaline's Consumer Driven Resource Types reuse the well defined query language of Facebook's GraphQL .","title":"Telling me what you need!"},{"location":"filters/#what-do-you-get-from-that","text":"With Consumer Driven Resource Types you can rely on the \"information hiding\" approach for experimental features, as clients not participating in the experiment would not be able to discover the experminatal feature by chance when playing with your API. And you can also securely remove deprecated features when you see that they are no longer in use. While this is in theory still a backward incompatible change, as long as it doesn't break anything, who cares? But there are more advantages that come with Consumer Driven Resource Types : By only responding what has been explicitly requested your clients can make best use of the available bandwidth to your API and you have eliminated the risk of non-functional backwards incompatibility introduced by additional large data elements. Rather than ignoring items of untyped collections they are not aware of, your clients tell you which types of items the can/want to handle. This helps avoiding situations where clients have to iterate through multiple pages of collections before they get the elements they can use. You can measure the popularity of each individual data element, which gives you a significant better understanding about your clients than you would have if you would only track the access on the level of resources.","title":"What do you get from that?"},{"location":"hypertext/","text":"Why Hypertext API? Although everyone these days is familiar with hypertext when browsing the web, in API hypertext formats are still quite uncommon. While most developers are used to URI as data-elements for accessing static resources (like jpeg photos), they are still new to using hypertext controls as mechanism to access the individual resources of an API. Also tool support for hypermedia controls in \"web frameworks\" is often still only limited. There still seems to be a need to convince both, client and server side developers (and their tool-producers) of the advantages of hypertext driven API. Why should API users want a hypertext API? Most developers are used to some hard coded URI templates in their application to construct the actual URI required to access an API resource. So many of them will argue that constructing the URI from values collected in the application context is a well understood problem with plenty of handy solutions available. So you better don't expect to get frenetic applause when you tell them that you now serve those much more convenient, ready to use hypermedia URI with the response. Best reaction you could expect is that your users don't care. But some might even dislike this, because their favorite \"REST toolkit\" expects URI templates for specific types of resources and doesn't work well with individual links per request. So be prepared that you need some convincing arguments! Changeable resource addressing schema The most obvious advantage of hypertext is the ability to change the resource addressing schema at any time. Unfortunately this looks like a very one-sided advantage for you as the provider of the API. From your user's point of view, they are not really interested in you being able to change your resource addressing schema. And it is hard to argue against users when they say that it should not be too difficult for an API provider to maintain legacy addressing schemas even when the underlying implementation dramatically changes. Worst case you have to (internally) map your legacy addressing schema to the most recent implementations. This argument misses one important aspect of the addressing schema: the domain name of the API request. As API evolve and grow, you might come to the point where you must apply some partitioning / sharding to your API. Either because you no longer can make all data available in your service efficiently available to all nodes, or because you realize that you must run different parts of your API on different machines. You could build some routing service in front of your API to overcome that problem, but not only does this additional network hub add unnecessary milliseconds to your processing time (and usually you do partitioning for performance reasons), it also introduces a new potential bottleneck in your architecture. Latest when you want to change networks for some parts of your API (e.g. when you want to run some aspects of your API through some CDN), the option for a routing service is gone, as it would kill the advantage of the partitioning. Application state in stateless service Even if you believe that you never have to partition you service, there is another important advantage that hypertext API enables: having application state in a stateless API implementation. Whoever has implemented a service that has to handle substantial load knows that things can get difficult as soon as you have maintain state specific to one client's flow though your service's business process implementation. If you want to maintain that state in your server side implementation, then you need often non-trivial mechanisms that ensure that all nodes in your cluster have access to that state-information. And once you have to plan for fail-over scenarios to other clusters / data centers things get really tricky. When you instead use hypertext, you get a nice, 100% fault-tolerant storage for the state information for free: the URI. While (practically) limited in size (although HTTP defines no size-restrictions for URI, some archaic, but still popular HTTP clients still have size constraints that limit for a general purpose API the URI to 2k), this is in many real-world cases sufficient space to encode state information you have to pass along. While feature-wise not equally powerful (server side state implementation ensure that state modifications will be shared with all API calls chronologically happen after the modification, while URI based state will only be available to resources that are linked directly or indirectly from the response that caused the modification), is is still sufficient in many cases. If that's what you need, you and your users get it for free. Even more important: you can introduce and change the availability and scope of state whenever needed. Often you only realize in an advanced version of your (stateless) API implementation, that information that was available in one step of your business process could provide improved results if also available in one of the subsequent steps. In a typical URI-template based API contract you would have no chance to pass this information along with existing clients, simply because it would require a change in existing client implementations to pass along that additional information. With hypertext you can do this kind of things, without breaking existing client implementations. Extensible business process flows With meshcaline 's flavor of hypertext you can enable one additional, very powerful feature: extensible business process flows! Having resource types as a core concept of a meshcaline design, an application could build extensible UX components, which can automatically (without any code change) offer new features to the their users as the feature-set of your API evolves: Assume our simple music service has four resource types: individual songs and artist ( #song , #artist ) and lists of those ( #songs , #artists ). Now suppose we define for each of the four resource types an attribute \"related\" and define it to hold a collection of hypertext controls, each containing a \"title\" element. E.g. in an early version of your API you offer a link to other popular songs of the same artist: { \"title\" : \"London Calling\", \"interpreter\" : { \"name\" : \"The Clash\", \"href\" : \"https:/....\", \"type\" : \"#artist\", ... }, \"related\": [ { \"title\": \"More songs of 'The Clash'\", \"href\":\"https:/...\", \"type\":\"#songs\" } ] } If application developers render in their UI some buttons displaying the title attribute and on selection of that button display a UI appropriate for resource-type, their applications will automatically offer additional features to their users when subsequent releases of your API introduce additional features: { \"title\" : \"London Calling\", \"interpreter\" : { \"name\" : \"The Clash\", \"href\" : \"https:/....\", \"type\" : \"#artist\", ... }, \"related\": [ { \"title\": \"More songs of 'The Clash'\", \"href\":\"https:/...\", \"type\":\"#songs\" }, { \"title\": \"Other British post-punk bands\", \"href\":\"https:/...\", \"type\":\"#artists\" }, { \"title\": \"Similar songs'\", \"href\":\"https:/...\", \"type\":\"#songs\" } ] } While in URI template based API your users would read you release notes to learn about the new features, check in documentation how to call those three different features and then implement some code that constructs the request, in a meshcaline API the amount of client code is primarily influenced by the number of resource types your API supports, not by the amount of API features. So as long as you don't introduce new result types, a forward compatible app implementation would have the new features automatically. As you might have seen the related collection contains hypertext controls for multiple resource types (both songs and #artists ). We could even introduce new resource types over time (e.g. #gigs ). All your user's applications have to be prepared for is to ignore any link with an unknown resource type (in the same way as they are used to ignore data attributes they don't know) and for the remaining ones let the user select one of the title elements and then pick an appropriate UI component for rendering the returned resource type. Such simple \"forward compatibility processing rules\" allow you to expand the features delivered by you API without being forced to introduce a new API version. In summary: A hypertext based API allows the API developers to improve and extend the implementation of their offering without breaking existing clients and allows API users to automatically gain from those improvements without the need to modify / reship their apps. Developer Experience Beyond the technical advantages of hypermedia, one of the most compelling reasons for a hypertext based public API offering is not technical at all: By using hypertext as interaction paradigm you can help your users to more easily understand your API, simply by using it within a browser. Even when you do nothing special but just serve your standard JSON representation, your users could just use one of the browser plugins that provide a nice JSON rendering and then see everything they can do with your API by playing around with your API in a browser and follow the links to other steps within the use-case flows. With some extra efforts you can even achieve significantly more: You could use standard HTML as representation format or, by supporting HTTP content negotiation, allow your users the select amongst various representation formats the one best suited for their needs. Then your API would automatically return the HTML representation when an API is entered in a browsers address field. Instead of / in addition to the JSON representation { \"title\" : \"London Calling\", \"interpreter\" : { \"name\" : \"The Clash\", \"href\" : \"https:/....\", \"type\" : \"#artist\", ... }, \"related\": [ { \"title\": \"More songs of 'The Clash'\", \"href\":\"https:/...\", \"type\":\"#songs\" } ] } you provide a HTML representation of your resource types: <!DOCTYPE html> <html> <body> <dl> <dt>title</dt><dd>London Calling</dd> <dt>interpreter</dt><dd> <a href=\"https:/....\"> <dl> <dt>name</dt><dd>The Clash</dd> <dt>type</dt><dd>#artist</dd> </dl> </a> </dd> <dt>related</dt><dd> <ul> <li> <a href=\"https:/....\"> <dl> <dt>title</dt><dd>More songs of 'The Clash'</dd> <dt>type</dt><dd>#songs</dd> </dl> </a> </li> </ul> </dd> </dl> </body> </html> While most developers would most likely still prefer to use a regular JSON/XML within their applications, supporting HTML representation in addition allows developers to immediately see all API features within a regular web browser. The can even directly follow all links with GET requests without additional work for you. By adding some JavaScript and CSS, you can then decorate your API with a fully fledged playground and turn it into a marketing tool / learning tool for your API: decorate resource types values in type and accept attributes with links to the corresponding API documentation sections use accept values to select widgets that allow to enter values required for following the hyperlink and construct the HTTP request use auth values to manage authorization needs and when required to display login widgets render a full dump of the request (e.g. by means of a curl statement) along to response representation provide a \"report a problem\" link along with every response. If your developers have a problem with / identify a bug in your API, they would simply fire the corresponding API call in a browser, then click on the report button, and the corresponding issue-reporting widget could then post your user's report along with the full HTTP request and the returned response into your issue tracking system provide UI widgets for rendering your resource types, that illustrate how the information returned could/should get displayed Contrary to most other playground solutions, this approach guarantees that the playground is always up-to-date with your API, as it is not a separate component independently developed from your API, but instead the API IS the playground. And still: the actual playground implementation can be nicely decoupled from your actual API code, maybe with some minor exceptions, like embedding JavaScript and CSS links into your HTML representation or some hacks required to overcome limitations of browsers (e.g. tunnel PUT/PATCH requests through POST, as browsers only support GET and POST in forms) Most modern web service frameworks provide support for multiple serialization formats and HTTP content negotiation, so building the initial support for multiple content types is doable with little extra efforts. Of course the actual implementation of the playground features like UI widgets in JavaScript and CSS can become quite some work -- depending on your own expectations and goals. What does hypertext bring to your business? All the advantages listed above are important for you business, as they reduce the cost of change, help you promoting/explaining your offering to potential users and whatever helps your users is also good for your API business. An additional, and maybe the most important aspect of hypertext based API on the long run is the possibility of a detailed analysis of your API usage. If your API is representing your business offering, you need to understand not only which resources of your API are popular, but also how your user's applications combine the various resources in their application flow. Take the efforts successful websites spend to analyze the behavior of their users and how they navigate through their website. I believe nobody disagrees that detailed understanding of user behavior and optimization around that is what differentiates a good website from a successful one. Now transfer this analogy from the website to your API: A classical, URI template based API is equivalent to a website where the only analytic tool you have available are page impression counts, as a HTTP based API (contrary to websites, where the browser implicitly sets the referrer header) lacks mechanisms that allow you to analyze the flow of the business process your users have taken. With a hypertext based API design instead where the URLs that are linking between the individual steps of a business process are nothing but meaningless opaque strings for the API user, you can easily encode additional information into the URI that allows you to track the detailed flow of your users (or better: the end-users of their applications). In case your API provides more that just a HTTP based interface to well-defined functionality, if you develop a solution where you have somewhere in your implementation to make the choice which of the possible responses might be the best / most appropriate for a given business process, or how to best rank the list of next possible steps, then detailed tracking of your API usage is mission critical for the incremental improvement of a good API towards a successful one. The collected usage data can become the strategic business asset securing your business success once other players come and try to copy your offering. You better don't rely on the hope that you're too clever for others to copy your innovation. If Gartner is right and information will become the oil of the 21st century , then you better start collecting data early, so that you can convert it into information required to stay successful in your domain. And if your API offering is in the heart of your business, then you better ensure that the tracking happens implicitly (through your links) and you don't rely on your API users to send back tracking information to you.","title":"Why Hypertext API"},{"location":"hypertext/#why-hypertext-api","text":"Although everyone these days is familiar with hypertext when browsing the web, in API hypertext formats are still quite uncommon. While most developers are used to URI as data-elements for accessing static resources (like jpeg photos), they are still new to using hypertext controls as mechanism to access the individual resources of an API. Also tool support for hypermedia controls in \"web frameworks\" is often still only limited. There still seems to be a need to convince both, client and server side developers (and their tool-producers) of the advantages of hypertext driven API.","title":"Why Hypertext API?"},{"location":"hypertext/#why-should-api-users-want-a-hypertext-api","text":"Most developers are used to some hard coded URI templates in their application to construct the actual URI required to access an API resource. So many of them will argue that constructing the URI from values collected in the application context is a well understood problem with plenty of handy solutions available. So you better don't expect to get frenetic applause when you tell them that you now serve those much more convenient, ready to use hypermedia URI with the response. Best reaction you could expect is that your users don't care. But some might even dislike this, because their favorite \"REST toolkit\" expects URI templates for specific types of resources and doesn't work well with individual links per request. So be prepared that you need some convincing arguments!","title":"Why should API users want a hypertext API?"},{"location":"hypertext/#changeable-resource-addressing-schema","text":"The most obvious advantage of hypertext is the ability to change the resource addressing schema at any time. Unfortunately this looks like a very one-sided advantage for you as the provider of the API. From your user's point of view, they are not really interested in you being able to change your resource addressing schema. And it is hard to argue against users when they say that it should not be too difficult for an API provider to maintain legacy addressing schemas even when the underlying implementation dramatically changes. Worst case you have to (internally) map your legacy addressing schema to the most recent implementations. This argument misses one important aspect of the addressing schema: the domain name of the API request. As API evolve and grow, you might come to the point where you must apply some partitioning / sharding to your API. Either because you no longer can make all data available in your service efficiently available to all nodes, or because you realize that you must run different parts of your API on different machines. You could build some routing service in front of your API to overcome that problem, but not only does this additional network hub add unnecessary milliseconds to your processing time (and usually you do partitioning for performance reasons), it also introduces a new potential bottleneck in your architecture. Latest when you want to change networks for some parts of your API (e.g. when you want to run some aspects of your API through some CDN), the option for a routing service is gone, as it would kill the advantage of the partitioning.","title":"Changeable resource addressing schema"},{"location":"hypertext/#application-state-in-stateless-service","text":"Even if you believe that you never have to partition you service, there is another important advantage that hypertext API enables: having application state in a stateless API implementation. Whoever has implemented a service that has to handle substantial load knows that things can get difficult as soon as you have maintain state specific to one client's flow though your service's business process implementation. If you want to maintain that state in your server side implementation, then you need often non-trivial mechanisms that ensure that all nodes in your cluster have access to that state-information. And once you have to plan for fail-over scenarios to other clusters / data centers things get really tricky. When you instead use hypertext, you get a nice, 100% fault-tolerant storage for the state information for free: the URI. While (practically) limited in size (although HTTP defines no size-restrictions for URI, some archaic, but still popular HTTP clients still have size constraints that limit for a general purpose API the URI to 2k), this is in many real-world cases sufficient space to encode state information you have to pass along. While feature-wise not equally powerful (server side state implementation ensure that state modifications will be shared with all API calls chronologically happen after the modification, while URI based state will only be available to resources that are linked directly or indirectly from the response that caused the modification), is is still sufficient in many cases. If that's what you need, you and your users get it for free. Even more important: you can introduce and change the availability and scope of state whenever needed. Often you only realize in an advanced version of your (stateless) API implementation, that information that was available in one step of your business process could provide improved results if also available in one of the subsequent steps. In a typical URI-template based API contract you would have no chance to pass this information along with existing clients, simply because it would require a change in existing client implementations to pass along that additional information. With hypertext you can do this kind of things, without breaking existing client implementations.","title":"Application state in stateless service"},{"location":"hypertext/#extensible-business-process-flows","text":"With meshcaline 's flavor of hypertext you can enable one additional, very powerful feature: extensible business process flows! Having resource types as a core concept of a meshcaline design, an application could build extensible UX components, which can automatically (without any code change) offer new features to the their users as the feature-set of your API evolves: Assume our simple music service has four resource types: individual songs and artist ( #song , #artist ) and lists of those ( #songs , #artists ). Now suppose we define for each of the four resource types an attribute \"related\" and define it to hold a collection of hypertext controls, each containing a \"title\" element. E.g. in an early version of your API you offer a link to other popular songs of the same artist: { \"title\" : \"London Calling\", \"interpreter\" : { \"name\" : \"The Clash\", \"href\" : \"https:/....\", \"type\" : \"#artist\", ... }, \"related\": [ { \"title\": \"More songs of 'The Clash'\", \"href\":\"https:/...\", \"type\":\"#songs\" } ] } If application developers render in their UI some buttons displaying the title attribute and on selection of that button display a UI appropriate for resource-type, their applications will automatically offer additional features to their users when subsequent releases of your API introduce additional features: { \"title\" : \"London Calling\", \"interpreter\" : { \"name\" : \"The Clash\", \"href\" : \"https:/....\", \"type\" : \"#artist\", ... }, \"related\": [ { \"title\": \"More songs of 'The Clash'\", \"href\":\"https:/...\", \"type\":\"#songs\" }, { \"title\": \"Other British post-punk bands\", \"href\":\"https:/...\", \"type\":\"#artists\" }, { \"title\": \"Similar songs'\", \"href\":\"https:/...\", \"type\":\"#songs\" } ] } While in URI template based API your users would read you release notes to learn about the new features, check in documentation how to call those three different features and then implement some code that constructs the request, in a meshcaline API the amount of client code is primarily influenced by the number of resource types your API supports, not by the amount of API features. So as long as you don't introduce new result types, a forward compatible app implementation would have the new features automatically. As you might have seen the related collection contains hypertext controls for multiple resource types (both songs and #artists ). We could even introduce new resource types over time (e.g. #gigs ). All your user's applications have to be prepared for is to ignore any link with an unknown resource type (in the same way as they are used to ignore data attributes they don't know) and for the remaining ones let the user select one of the title elements and then pick an appropriate UI component for rendering the returned resource type. Such simple \"forward compatibility processing rules\" allow you to expand the features delivered by you API without being forced to introduce a new API version. In summary: A hypertext based API allows the API developers to improve and extend the implementation of their offering without breaking existing clients and allows API users to automatically gain from those improvements without the need to modify / reship their apps.","title":"Extensible business process flows"},{"location":"hypertext/#developer-experience","text":"Beyond the technical advantages of hypermedia, one of the most compelling reasons for a hypertext based public API offering is not technical at all: By using hypertext as interaction paradigm you can help your users to more easily understand your API, simply by using it within a browser. Even when you do nothing special but just serve your standard JSON representation, your users could just use one of the browser plugins that provide a nice JSON rendering and then see everything they can do with your API by playing around with your API in a browser and follow the links to other steps within the use-case flows. With some extra efforts you can even achieve significantly more: You could use standard HTML as representation format or, by supporting HTTP content negotiation, allow your users the select amongst various representation formats the one best suited for their needs. Then your API would automatically return the HTML representation when an API is entered in a browsers address field. Instead of / in addition to the JSON representation { \"title\" : \"London Calling\", \"interpreter\" : { \"name\" : \"The Clash\", \"href\" : \"https:/....\", \"type\" : \"#artist\", ... }, \"related\": [ { \"title\": \"More songs of 'The Clash'\", \"href\":\"https:/...\", \"type\":\"#songs\" } ] } you provide a HTML representation of your resource types: <!DOCTYPE html> <html> <body> <dl> <dt>title</dt><dd>London Calling</dd> <dt>interpreter</dt><dd> <a href=\"https:/....\"> <dl> <dt>name</dt><dd>The Clash</dd> <dt>type</dt><dd>#artist</dd> </dl> </a> </dd> <dt>related</dt><dd> <ul> <li> <a href=\"https:/....\"> <dl> <dt>title</dt><dd>More songs of 'The Clash'</dd> <dt>type</dt><dd>#songs</dd> </dl> </a> </li> </ul> </dd> </dl> </body> </html> While most developers would most likely still prefer to use a regular JSON/XML within their applications, supporting HTML representation in addition allows developers to immediately see all API features within a regular web browser. The can even directly follow all links with GET requests without additional work for you. By adding some JavaScript and CSS, you can then decorate your API with a fully fledged playground and turn it into a marketing tool / learning tool for your API: decorate resource types values in type and accept attributes with links to the corresponding API documentation sections use accept values to select widgets that allow to enter values required for following the hyperlink and construct the HTTP request use auth values to manage authorization needs and when required to display login widgets render a full dump of the request (e.g. by means of a curl statement) along to response representation provide a \"report a problem\" link along with every response. If your developers have a problem with / identify a bug in your API, they would simply fire the corresponding API call in a browser, then click on the report button, and the corresponding issue-reporting widget could then post your user's report along with the full HTTP request and the returned response into your issue tracking system provide UI widgets for rendering your resource types, that illustrate how the information returned could/should get displayed Contrary to most other playground solutions, this approach guarantees that the playground is always up-to-date with your API, as it is not a separate component independently developed from your API, but instead the API IS the playground. And still: the actual playground implementation can be nicely decoupled from your actual API code, maybe with some minor exceptions, like embedding JavaScript and CSS links into your HTML representation or some hacks required to overcome limitations of browsers (e.g. tunnel PUT/PATCH requests through POST, as browsers only support GET and POST in forms) Most modern web service frameworks provide support for multiple serialization formats and HTTP content negotiation, so building the initial support for multiple content types is doable with little extra efforts. Of course the actual implementation of the playground features like UI widgets in JavaScript and CSS can become quite some work -- depending on your own expectations and goals.","title":"Developer Experience"},{"location":"hypertext/#what-does-hypertext-bring-to-your-business","text":"All the advantages listed above are important for you business, as they reduce the cost of change, help you promoting/explaining your offering to potential users and whatever helps your users is also good for your API business. An additional, and maybe the most important aspect of hypertext based API on the long run is the possibility of a detailed analysis of your API usage. If your API is representing your business offering, you need to understand not only which resources of your API are popular, but also how your user's applications combine the various resources in their application flow. Take the efforts successful websites spend to analyze the behavior of their users and how they navigate through their website. I believe nobody disagrees that detailed understanding of user behavior and optimization around that is what differentiates a good website from a successful one. Now transfer this analogy from the website to your API: A classical, URI template based API is equivalent to a website where the only analytic tool you have available are page impression counts, as a HTTP based API (contrary to websites, where the browser implicitly sets the referrer header) lacks mechanisms that allow you to analyze the flow of the business process your users have taken. With a hypertext based API design instead where the URLs that are linking between the individual steps of a business process are nothing but meaningless opaque strings for the API user, you can easily encode additional information into the URI that allows you to track the detailed flow of your users (or better: the end-users of their applications). In case your API provides more that just a HTTP based interface to well-defined functionality, if you develop a solution where you have somewhere in your implementation to make the choice which of the possible responses might be the best / most appropriate for a given business process, or how to best rank the list of next possible steps, then detailed tracking of your API usage is mission critical for the incremental improvement of a good API towards a successful one. The collected usage data can become the strategic business asset securing your business success once other players come and try to copy your offering. You better don't rely on the hope that you're too clever for others to copy your innovation. If Gartner is right and information will become the oil of the 21st century , then you better start collecting data early, so that you can convert it into information required to stay successful in your domain. And if your API offering is in the heart of your business, then you better ensure that the tracking happens implicitly (through your links) and you don't rely on your API users to send back tracking information to you.","title":"What does hypertext bring to your business?"},{"location":"legal/","text":"Legal Notice Unless stated otherwise all content on meshcaline.org is copyright by Andreas Schmidt and licensed under a Creative Commons Attribution 4.0 International License . The photo on the cover page is copyright by Keith Marshall","title":"Legal Notice"},{"location":"legal/#legal-notice","text":"Unless stated otherwise all content on meshcaline.org is copyright by Andreas Schmidt and licensed under a Creative Commons Attribution 4.0 International License . The photo on the cover page is copyright by Keith Marshall","title":"Legal Notice"},{"location":"meshcaline/","text":"meshcaline? There are only two hard things in Computer Science: cache invalidation and naming things. -- Phil Karlton You wonder where the neologism meshcaline comes from? It has two origins: mesh : I believe that a mesh is an appropriate visual representation for the concept of a hypertext driven API: While the real power of an API is provided by its individual resources (the mesh) what you see from the whole thing are primarily the edges (the links) mescaline : This psychedelic substance occurs naturally in the peyote cactus and has been used from earliest recorded time for medical and cultural purposes. In the 1960s the drug was quite popular amongst people that wanted to find a different truth beyond the obvious reality. While drugs definitively won't help you in designing a good API, questioning the given and seeking for the better is a trait good API designers have. Being listed in the US Dispensatory to help against hysteria seems to be another allusion to the need not to blindly follow each trend, regardless how many protagonists there may be. Beyond the nice analogies to this project the word has some additional properties that seemed to fit quite well: The word meshcaline is both a noun (naming the concept) as well as an adjective (describing the individual characteristics of a design) Some people immediately had the association masculine . While I hope that female developers find the concepts equally appealing, having broad shoulders (metaphorically speaking) helps when you argue against popular wisdom. Being associated with a drug hopefully prevents the word to become a popular marketing term, so that people who use it, know why and what they use it for. If more people find the ideas in this blog worth to work on/with, we can form the tribe of meshcaleros","title":"meshcaline?"},{"location":"meshcaline/#meshcaline","text":"There are only two hard things in Computer Science: cache invalidation and naming things. -- Phil Karlton You wonder where the neologism meshcaline comes from? It has two origins: mesh : I believe that a mesh is an appropriate visual representation for the concept of a hypertext driven API: While the real power of an API is provided by its individual resources (the mesh) what you see from the whole thing are primarily the edges (the links) mescaline : This psychedelic substance occurs naturally in the peyote cactus and has been used from earliest recorded time for medical and cultural purposes. In the 1960s the drug was quite popular amongst people that wanted to find a different truth beyond the obvious reality. While drugs definitively won't help you in designing a good API, questioning the given and seeking for the better is a trait good API designers have. Being listed in the US Dispensatory to help against hysteria seems to be another allusion to the need not to blindly follow each trend, regardless how many protagonists there may be. Beyond the nice analogies to this project the word has some additional properties that seemed to fit quite well: The word meshcaline is both a noun (naming the concept) as well as an adjective (describing the individual characteristics of a design) Some people immediately had the association masculine . While I hope that female developers find the concepts equally appealing, having broad shoulders (metaphorically speaking) helps when you argue against popular wisdom. Being associated with a drug hopefully prevents the word to become a popular marketing term, so that people who use it, know why and what they use it for. If more people find the ideas in this blog worth to work on/with, we can form the tribe of meshcaleros","title":"meshcaline?"},{"location":"pagination/","text":"Paginateable Collections Often resources can contain collections of objects, or collections of hypertext controls to other resources, where the size of the collections would make it a bad idea to return the full collection in one API response. Except in cases where it is known upfront that a specific collection can never grow beyond some reasonable size, a meshcaline API should use a standardized pattern for paginateable collections. This representation then holds at least two elements: the individual items of the first page of the collection in an attribute items and an optional hypertext control next that points to the next page: { \"name\" : \"The Clash\", \"popularSongs\": { \"next\": \"http://....\", \"items\": [ { \"title\" : \"London Calling\", \"href\": \"http://\", \"type\": \"#song\", ... }, { ... }, ... ] }, ... } Hypertext controls for pagination have a resource type #implied and a method GET and therefore the controls can always use the simplified URI string representation. Following such a link does return the resource type implicitly defined from the link's context. In our example it contains the next chunk of items of the popularSongs collection: { \"next\": \"http://....\", \"items\": [ { \"title\" : \"Rock the Casbah\", \"href\": \"http://\", \"type\": \"#song\", ... }, ... ] } The end of the collection is reached, when there is no more next attribute. The specific representations of the items attribute may vary, depending on the type of collection (Maps, Lists, Sets). A meshcaline API may provide additional attributes in their collection representation. The usual suspects include: the total number of available items, some offset of the first item in the current page in the whole a collection, or additional hypertext controls for direct access to the previous, first or last page of a collection. You better think twice before you introduce such convenience features: Your API users will expect a consistent set of collection attributes for all collections in your API. And while you may initially think this is not too hard to achieve, there exist classes of collections where this is not simple, if not impossible: Assume we want to add a list of next concerts to our artist representation, but not having such data yourself you rely for that feature on a ticket supplier's API that only allows you to fetch the next 10 gigs after a given date. Then you would neither know how many concerts will be available in total, nor can you easily build a prev link, unless you encode somehow the dates of all previously visited pages in the URI. A last link would even require you to iterate the 3rd party API until you get the first response with less than 10 results back. Therefore it is best practice to keep the pagination representation minimal unless you learn from your users that more complex pagination constructs are really required for the business processes your API shall support.","title":"Paginateable Collections"},{"location":"pagination/#paginateable-collections","text":"Often resources can contain collections of objects, or collections of hypertext controls to other resources, where the size of the collections would make it a bad idea to return the full collection in one API response. Except in cases where it is known upfront that a specific collection can never grow beyond some reasonable size, a meshcaline API should use a standardized pattern for paginateable collections. This representation then holds at least two elements: the individual items of the first page of the collection in an attribute items and an optional hypertext control next that points to the next page: { \"name\" : \"The Clash\", \"popularSongs\": { \"next\": \"http://....\", \"items\": [ { \"title\" : \"London Calling\", \"href\": \"http://\", \"type\": \"#song\", ... }, { ... }, ... ] }, ... } Hypertext controls for pagination have a resource type #implied and a method GET and therefore the controls can always use the simplified URI string representation. Following such a link does return the resource type implicitly defined from the link's context. In our example it contains the next chunk of items of the popularSongs collection: { \"next\": \"http://....\", \"items\": [ { \"title\" : \"Rock the Casbah\", \"href\": \"http://\", \"type\": \"#song\", ... }, ... ] } The end of the collection is reached, when there is no more next attribute. The specific representations of the items attribute may vary, depending on the type of collection (Maps, Lists, Sets). A meshcaline API may provide additional attributes in their collection representation. The usual suspects include: the total number of available items, some offset of the first item in the current page in the whole a collection, or additional hypertext controls for direct access to the previous, first or last page of a collection. You better think twice before you introduce such convenience features: Your API users will expect a consistent set of collection attributes for all collections in your API. And while you may initially think this is not too hard to achieve, there exist classes of collections where this is not simple, if not impossible: Assume we want to add a list of next concerts to our artist representation, but not having such data yourself you rely for that feature on a ticket supplier's API that only allows you to fetch the next 10 gigs after a given date. Then you would neither know how many concerts will be available in total, nor can you easily build a prev link, unless you encode somehow the dates of all previously visited pages in the URI. A last link would even require you to iterate the 3rd party API until you get the first response with less than 10 results back. Therefore it is best practice to keep the pagination representation minimal unless you learn from your users that more complex pagination constructs are really required for the business processes your API shall support.","title":"Paginateable Collections"},{"location":"permalinks/","text":"API Permalinks Client applications of a meshcaline API must consider all resources accessed during a business process as \"temporary\" resources. The validity / accessibility of each resource is bound to what the API developer defines as appropriate duration for one business process. But business processes are usually not strictly isolated concepts. Often one business process (e.g. searching for some object) leads to some resource (e.g. the object searched for) from where you can start new business processes (e.g. update the object). As a result you might have resources for which you want to allow your application developers to keep permanent references so that their users can start a new business process in a future session from that resource. E.g. in our music API we may want to allow our customer's applications to store lists of favorite songs for their end-users lists which those might have discovered in various business processes (e.g. a search business process, recommendation business processes, etc.). In those cases the response representation contains a hypertext control with link-relation bookmark . Those hypertext controls will usually use the simplified URI-string representation: { \"bookmark\" : \"http://..\", \"title\": \"London Calling\", \"interpreter\": { \"name\": \"The Clash\", \"href\": \"http:/....\", \"type\": \"#artist\", ... }, ... } You should not introduce permalinks for every resource, but only for those where you have a reason. With permalinks you give a long term promise not only to the availability of the resource, but also to the addressing schema used in the URI. While you don't have to disclose the addressing schema directly, you still must ensure that all subsequent iterations of your offering will still understand all addressing schemas ever used for bookmarks. When you track your business process steps through your URI schema you can encode additional information in your bookmark URI (e.g. an identifier of the flow and a time-stamp indicating when the bookmark was created). This allows your analytics jobs to associate the new use-case flow initiated through the bookmark with the one that created the bookmark. Requests using such a bookmark URI can then return a temporary redirect to the actual resource URI, and this URL will then encode the identifier of the new business process initiated by the bookmark URL.","title":"API Permalinks"},{"location":"permalinks/#api-permalinks","text":"Client applications of a meshcaline API must consider all resources accessed during a business process as \"temporary\" resources. The validity / accessibility of each resource is bound to what the API developer defines as appropriate duration for one business process. But business processes are usually not strictly isolated concepts. Often one business process (e.g. searching for some object) leads to some resource (e.g. the object searched for) from where you can start new business processes (e.g. update the object). As a result you might have resources for which you want to allow your application developers to keep permanent references so that their users can start a new business process in a future session from that resource. E.g. in our music API we may want to allow our customer's applications to store lists of favorite songs for their end-users lists which those might have discovered in various business processes (e.g. a search business process, recommendation business processes, etc.). In those cases the response representation contains a hypertext control with link-relation bookmark . Those hypertext controls will usually use the simplified URI-string representation: { \"bookmark\" : \"http://..\", \"title\": \"London Calling\", \"interpreter\": { \"name\": \"The Clash\", \"href\": \"http:/....\", \"type\": \"#artist\", ... }, ... } You should not introduce permalinks for every resource, but only for those where you have a reason. With permalinks you give a long term promise not only to the availability of the resource, but also to the addressing schema used in the URI. While you don't have to disclose the addressing schema directly, you still must ensure that all subsequent iterations of your offering will still understand all addressing schemas ever used for bookmarks. When you track your business process steps through your URI schema you can encode additional information in your bookmark URI (e.g. an identifier of the flow and a time-stamp indicating when the bookmark was created). This allows your analytics jobs to associate the new use-case flow initiated through the bookmark with the one that created the bookmark. Requests using such a bookmark URI can then return a temporary redirect to the actual resource URI, and this URL will then encode the identifier of the new business process initiated by the bookmark URL.","title":"API Permalinks"},{"location":"privacy/","text":"Privacy Policy The use of the Internet pages of meshcaline.org is possible without any indication of personal data; however, if a data subject wants to use special enterprise services via our website, processing of personal data could become necessary. If the processing of personal data is necessary and there is no statutory basis for such processing, we generally obtain consent from the data subject. The processing of personal data, such as the name, address, e-mail address, or telephone number of a data subject shall always be in line with the General Data Protection Regulation (GDPR), and in accordance with the country-specific data protection regulations applicable to meshcaline.org. By means of this data protection declaration, meshcaline.org would like to inform the general public of the nature, scope, and purpose of the personal data we collect, use and process. Furthermore, data subjects are informed, by means of this data protection declaration, of the rights to which they are entitled. As the controller, meshcaline.org has implemented numerous technical and organizational measures to ensure the most complete protection of personal data processed through this website. However, Internet-based data transmissions may in principle have security gaps, so absolute protection may not be guaranteed. For this reason, every data subject is free to transfer personal data to meshcaline.org via alternative means, e.g. by mail. Definitions The data protection declaration of meshcaline.org is based on the terms used by the European legislator for the adoption of the General Data Protection Regulation (GDPR). Our data protection declaration should be legible and understandable for the general public, as well as our customers and business partners. To ensure this, we would like to first explain the terminology used. In this data protection declaration, we use, inter alia, the following terms: Personal data Personal data means any information relating to an identified or identifiable natural person (\u201cdata subject\u201d). An identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person. Data subject Data subject is any identified or identifiable natural person, whose personal data is processed by the controller responsible for the processing. Processing Processing is any operation or set of operations which is performed on personal data or on sets of personal data, whether or not by automated means, such as collection, recording, organisation, structuring, storage, adaptation or alteration, retrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure or destruction. Restriction of processing Restriction of processing is the marking of stored personal data with the aim of limiting their processing in the future. Profiling Profiling means any form of automated processing of personal data consisting of the use of personal data to evaluate certain personal aspects relating to a natural person, in particular to analyse or predict aspects concerning that natural person's performance at work, economic situation, health, personal preferences, interests, reliability, behaviour, location or movements. Pseudonymisation Pseudonymisation is the processing of personal data in such a manner that the personal data can no longer be attributed to a specific data subject without the use of additional information, provided that such additional information is kept separately and is subject to technical and organisational measures to ensure that the personal data are not attributed to an identified or identifiable natural person. Controller or controller responsible for the processing Controller or controller responsible for the processing is the natural or legal person, public authority, agency or other body which, alone or jointly with others, determines the purposes and means of the processing of personal data; where the purposes and means of such processing are determined by Union or Member State law, the controller or the specific criteria for its nomination may be provided for by Union or Member State law. Processor Processor is a natural or legal person, public authority, agency or other body which processes personal data on behalf of the controller. Recipient Recipient is a natural or legal person, public authority, agency or another body, to which the personal data are disclosed, whether a third party or not. However, public authorities which may receive personal data in the framework of a particular inquiry in accordance with Union or Member State law shall not be regarded as recipients; the processing of those data by those public authorities shall be in compliance with the applicable data protection rules according to the purposes of the processing. Third party Third party is a natural or legal person, public authority, agency or body other than the data subject, controller, processor and persons who, under the direct authority of the controller or processor, are authorised to process personal data. Consent Consent of the data subject is any freely given, specific, informed and unambiguous indication of the data subject's wishes by which he or she, by a statement or by a clear affirmative action, signifies agreement to the processing of personal data relating to him or her. Name and address of the controller Controller for the purposes of the General Data Protection Regulation (GDPR), other data protection laws applicable in Member states of the European Union and other provisions related to data protection is: Andreas Schmidt Mittenwalder Str. 19 10961 Berlin Germany if( window.location.pathname==\"/about/\" ){ var t=\"org\", d=\"meshcaline\", n=\"meshcalero\"; var m=n+\"@\"+d+\".\"+t; document.write('Email: <a href=\"mailto:'+m+'\">'+m+'</a>');} Website: http://blog.meshcaline.org Collection of general data and information The website meshcaline.org collects a series of general data and information when a data subject or automated system calls up the website. This general data and information are stored in the server log files. Collected may be the browser types and versions used, the operating system used by the accessing system, the website from which an accessing system reaches our website (so-called referrers), the sub-websites, the date and time of access to the Internet site, an anonymized Internet protocol address (IP address), the Internet service provider of the accessing system, and any other similar data and information that may be used in the event of attacks on our information technology systems. When using these general data and information, meshcaline.org does not draw any conclusions about the data subject. Rather, this information is needed to deliver the content of our website correctly, optimize the content of our website as well as its advertisement, ensure the long-term viability of our information technology systems and website technology, and provide law enforcement authorities with the information necessary for criminal prosecution in case of a cyber-attack. Therefore, meshcaline.org analyzes anonymously collected data and information statistically, with the aim of increasing the data protection and data security of our website, and to ensure an optimal level of protection for the personal data we process. The anonymous data of the server log files are stored separately from all personal data provided by a data subject. Routine erasure and blocking of personal data The data controller shall process and store the personal data of the data subject only for the period necessary to achieve the purpose of storage, or as far as this is granted by the European legislator or other legislators in laws or regulations to which the controller is subject to. If the storage purpose is not applicable, or if a storage period prescribed by the European legislator or another competent legislator expires, the personal data are routinely blocked or erased in accordance with legal requirements. Rights of the data subject Right of confirmation Each data subject shall have the right granted by the European legislator to obtain from the controller the confirmation as to whether or not personal data concerning him or her are being processed. If a data subject wishes to avail himself of this right of confirmation, he or she may, at any time, contact any employee of the controller. Right of access Each data subject shall have the right granted by the European legislator to obtain from the controller free information about his or her personal data stored at any time and a copy of this information. Furthermore, the European directives and regulations grant the data subject access to the following information: * the purposes of the processing; * the categories of personal data concerned; * the recipients or categories of recipients to whom the personal data have been or will be disclosed, in particular recipients in third countries or international organisations; * where possible, the envisaged period for which the personal data will be stored, or, if not possible, the criteria used to determine that period; * the existence of the right to request from the controller rectification or erasure of personal data, or restriction of processing of personal data concerning the data subject, or to object to such processing; * the existence of the right to lodge a complaint with a supervisory authority; * where the personal data are not collected from the data subject, any available information as to their source; * the existence of automated decision-making, including profiling, referred to in Article 22(1) and (4) of the GDPR and, at least in those cases, meaningful information about the logic involved, as well as the significance and envisaged consequences of such processing for the data subject. Furthermore, the data subject shall have a right to obtain information as to whether personal data are transferred to a third country or to an international organisation. Where this is the case, the data subject shall have the right to be informed of the appropriate safeguards relating to the transfer. If a data subject wishes to avail himself of this right of access, he or she may, at any time, contact any employee of the controller. Right to rectification Each data subject shall have the right granted by the European legislator to obtain from the controller without undue delay the rectification of inaccurate personal data concerning him or her. Taking into account the purposes of the processing, the data subject shall have the right to have incomplete personal data completed, including by means of providing a supplementary statement. If a data subject wishes to exercise this right to rectification, he or she may, at any time, contact any employee of the controller. Right to erasure (Right to be forgotten) Each data subject shall have the right granted by the European legislator to obtain from the controller the erasure of personal data concerning him or her without undue delay, and the controller shall have the obligation to erase personal data without undue delay where one of the following grounds applies, as long as the processing is not necessary: The personal data are no longer necessary in relation to the purposes for which they were collected or otherwise processed. The data subject withdraws consent to which the processing is based according to point (a) of Article 6(1) of the GDPR, or point (a) of Article 9(2) of the GDPR, and where there is no other legal ground for the processing. The data subject objects to the processing pursuant to Article 21(1) of the GDPR and there are no overriding legitimate grounds for the processing, or the data subject objects to the processing pursuant to Article 21(2) of the GDPR. The personal data have been unlawfully processed. The personal data must be erased for compliance with a legal obligation in Union or Member State law to which the controller is subject. The personal data have been collected in relation to the offer of information society services referred to in Article 8(1) of the GDPR. If one of the aforementioned reasons applies, and a data subject wishes to request the erasure of personal data stored by meshcaline.org, he or she may, at any time, contact the controller. The controller shall promptly ensure that the erasure request is complied with immediately. Where the controller has made personal data public and is obliged pursuant to Article 17(1) to erase the personal data, the controller, taking account of available technology and the cost of implementation, shall take reasonable steps, including technical measures, to inform other controllers processing the personal data that the data subject has requested erasure by such controllers of any links to, or copy or replication of, those personal data, as far as processing is not required. meshcaline.org will arrange the necessary measures in individual cases. Right of restriction of processing Each data subject shall have the right granted by the European legislator to obtain from the controller restriction of processing where one of the following applies: * The accuracy of the personal data is contested by the data subject, for a period enabling the controller to verify the accuracy of the personal data. * The processing is unlawful and the data subject opposes the erasure of the personal data and requests instead the restriction of their use instead. * The controller no longer needs the personal data for the purposes of the processing, but they are required by the data subject for the establishment, exercise or defence of legal claims. * The data subject has objected to processing pursuant to Article 21(1) of the GDPR pending the verification whether the legitimate grounds of the controller override those of the data subject. If one of the aforementioned conditions is met, and a data subject wishes to request the restriction of the processing of personal data stored by meshcaline.org, he or she may at any time contact meshcaline.org. meshcaline.org will arrange the restriction of the processing. Right to data portability Each data subject shall have the right granted by the European legislator, to receive the personal data concerning him or her, which was provided to a controller, in a structured, commonly used and machine-readable format. He or she shall have the right to transmit those data to another controller without hindrance from the controller to which the personal data have been provided, as long as the processing is based on consent pursuant to point (a) of Article 6(1) of the GDPR or point (a) of Article 9(2) of the GDPR, or on a contract pursuant to point (b) of Article 6(1) of the GDPR, and the processing is carried out by automated means, as long as the processing is not necessary for the performance of a task carried out in the public interest or in the exercise of official authority vested in the controller. Furthermore, in exercising his or her right to data portability pursuant to Article 20(1) of the GDPR, the data subject shall have the right to have personal data transmitted directly from one controller to another, where technically feasible and when doing so does not adversely affect the rights and freedoms of others. In order to assert the right to data portability, the data subject may at any time contact the controller. Right to object Each data subject shall have the right granted by the European legislator to object, on grounds relating to his or her particular situation, at any time, to processing of personal data concerning him or her, which is based on point (e) or (f) of Article 6(1) of the GDPR. This also applies to profiling based on these provisions. meshcaline.org shall no longer process the personal data in the event of the objection, unless we can demonstrate compelling legitimate grounds for the processing which override the interests, rights and freedoms of the data subject, or for the establishment, exercise or defence of legal claims. If meshcaline.org processes personal data for direct marketing purposes, the data subject shall have the right to object at any time to processing of personal data concerning him or her for such marketing. This applies to profiling to the extent that it is related to such direct marketing. If the data subject objects to meshcaline.org to the processing for direct marketing purposes, meshcaline.org will no longer process the personal data for these purposes. In addition, the data subject has the right, on grounds relating to his or her particular situation, to object to processing of personal data concerning him or her by meshcaline.org for scientific or historical research purposes, or for statistical purposes pursuant to Article 89(1) of the GDPR, unless the processing is necessary for the performance of a task carried out for reasons of public interest. In order to exercise the right to object, the data subject may contact meshcaline.org. In addition, the data subject is free in the context of the use of information society services, and notwithstanding Directive 2002/58/EC, to use his or her right to object by automated means using technical specifications. Automated individual decision-making, including profiling Each data subject shall have the right granted by the European legislator not to be subject to a decision based solely on automated processing, including profiling, which produces legal effects concerning him or her, or similarly significantly affects him or her, as long as the decision (1) is not is necessary for entering into, or the performance of, a contract between the data subject and a data controller, or (2) is not authorised by Union or Member State law to which the controller is subject and which also lays down suitable measures to safeguard the data subject's rights and freedoms and legitimate interests, or (3) is not based on the data subject's explicit consent. If the decision (1) is necessary for entering into, or the performance of, a contract between the data subject and a data controller, or (2) it is based on the data subject's explicit consent, meshcaline.org shall implement suitable measures to safeguard the data subject's rights and freedoms and legitimate interests, at least the right to obtain human intervention on the part of the controller, to express his or her point of view and contest the decision. If the data subject wishes to exercise the rights concerning automated individual decision-making, he or she may, at any time, contact meshcaline.org. Right to withdraw data protection consent Each data subject shall have the right granted by the European legislator to withdraw his or her consent to processing of his or her personal data at any time. If the data subject wishes to exercise the right to withdraw the consent, he or she may, at any time, contact meshcaline.org. Legal basis for the processing Art. 6(1) lit. a GDPR serves as the legal basis for processing operations for which we obtain consent for a specific processing purpose. If the processing of personal data is necessary for the performance of a contract to which the data subject is party, as is the case, for example, when processing operations are necessary for the supply of goods or to provide any other service, the processing is based on Article 6(1) lit. b GDPR. The same applies to such processing operations which are necessary for carrying out pre-contractual measures, for example in the case of inquiries concerning our products or services. Is our company subject to a legal obligation by which processing of personal data is required, such as for the fulfillment of tax obligations, the processing is based on Art. 6(1) lit. c GDPR. In rare cases, the processing of personal data may be necessary to protect the vital interests of the data subject or of another natural person. This would be the case, for example, if a visitor were injured in our company and his name, age, health insurance data or other vital information would have to be passed on to a doctor, hospital or other third party. Then the processing would be based on Art. 6(1) lit. d GDPR. Finally, processing operations could be based on Article 6(1) lit. f GDPR. This legal basis is used for processing operations which are not covered by any of the abovementioned legal grounds, if processing is necessary for the purposes of the legitimate interests pursued by our company or by a third party, except where such interests are overridden by the interests or fundamental rights and freedoms of the data subject which require protection of personal data. Such processing operations are particularly permissible because they have been specifically mentioned by the European legislator. He considered that a legitimate interest could be assumed if the data subject is a client of the controller (Recital 47 Sentence 2 GDPR). The legitimate interests pursued by the controller or by a third party Where the processing of personal data is based on Article 6(1) lit. f GDPR our legitimate interest is to being able to offer meshcaline.org to interested readers and continuously improve the service. Period for which the personal data will be stored The criteria used to determine the period of storage of personal data is the respective statutory retention period. After expiration of that period, the corresponding data is routinely deleted, as long as it is no longer necessary for the fulfillment of the contract or the initiation of a contract. Provision of personal data as statutory or contractual requirement; Requirement necessary to enter into a contract; Obligation of the data subject to provide the personal data; possible consequences of failure to provide such data We clarify that the provision of personal data is partly required by law (e.g. tax regulations) or can also result from contractual provisions (e.g. information on the contractual partner). Sometimes it may be necessary to conclude a contract that the data subject provides us with personal data, which must subsequently be processed by us. The data subject is, for example, obliged to provide us with personal data when our company signs a contract with him or her. The non-provision of the personal data would have the consequence that the contract with the data subject could not be concluded. Before personal data is provided by the data subject, the data subject must contact the controller. The controller clarifies to the data subject whether the provision of the personal data is required by law or contract or is necessary for the conclusion of the contract, whether there is an obligation to provide the personal data and the consequences of non-provision of the personal data. Existence of automated decision-making As a responsible controller, we do not use automatic decision-making or profiling. 3rd Parties Hosting The hosting services we use provide the following services: infrastructure and platform services, computing capacity, storage and database services, security and technical maintenance services we use to operate this online service. Here we, or our hosting provider, process inventory data, contact data, content data, contract data, usage data, meta and communication data of customers, interested parties and visitors to this online offer on the basis of our legitimate interests in an efficient and secure provision of this online offer acc. art. 6 para. 1 lit. f GDPR in combination with art. 28 GDPR. DISQUS Comment Function Based on our legitimate interestson on an efficient, secure and user-friendly comment management acc. Art. 6 para. 1 lit. f. GDPR we rely on the commenting service DISQUS offered by DISQUS, Inc., 301 Howard St, Floor 3, San Francisco, California 94105, USA. DISQUS is certified under the Privacy Shield Agreement, which provides a guarantee to comply with European privacy legislation: https://www.privacyshield.gov/participant?id=a2zt0000000TRkEAAW&status=Active To use the DISQUS comment feature, users can log in using their own DISQUS user account or existing social media accounts (e.g., OpenID, Facebook, Twitter, or Google). Here, the user credentials are obtained from the platforms through DISQS. It is also possible to use the DISQUS comment feature as a guest without creating or using user accounts with DISQUS or any of the social media providers listed. We only embed DISQUS with its features in our website, and we can influence the comments of the users. However, users enter into a direct contractual relationship with DISQUS, in which DISQS processes users 'comments and acts as point of contact for any deletion of users' data. We refer to the privacy policy of DISQUS: https://help.disqus.com/terms-and-policies/disqus-privacy-policy and also inform the users that they can assume that DISQUS not only stores the comment content but also their IP address and the time of the comment, as well stores cookies on the computers of users and can use this data to display advertising. However, users may object to the processing of their data to display ads: https://disqus.com/data-sharing-settings Google Analytics On this website, the controller has integrated the component of Google Analytics (with the anonymizer function). Google Analytics is a web analytics service. Web analytics is the collection, gathering, and analysis of data about the behavior of visitors to websites. A web analysis service collects, inter alia, data about the website from which a person has come (the so-called referrer), which sub-pages were visited, or how often and for what duration a sub-page was viewed. Web analytics are mainly used for the optimization of a website and in order to carry out a cost-benefit analysis of Internet advertising. The operator of the Google Analytics component is Google Inc., 1600 Amphitheatre Pkwy, Mountain View, CA 94043-1351, United States. For the web analytics through Google Analytics the controller uses IP Anonymization in Analytics . By means of this feature the IP address of the Internet connection of the data subject is abridged by Google and anonymised when accessing our websites from a Member State of the European Union or another Contracting State to the Agreement on the European Economic Area. The purpose of the Google Analytics component is to analyze the traffic on our website. Google uses the collected data and information, inter alia, to evaluate the use of our website and to provide online reports, which show the activities on our websites, and to provide other services concerning the use of our Internet site for us. Google Analytics places a cookie on the information technology system of the data subject. The definition of cookies is explained above. With the setting of the cookie, Google is enabled to analyze the use of our website. With each call-up to one of the individual pages of this Internet site, which is operated by the controller and into which a Google Analytics component was integrated, the Internet browser on the information technology system of the data subject will automatically submit data through the Google Analytics component for the purpose of online advertising and the settlement of commissions to Google. During the course of this technical procedure, the enterprise Google gains knowledge of personal information, such as the IP address of the data subject, which serves Google, inter alia, to understand the origin of visitors and clicks, and subsequently create commission settlements. The cookie is used to store personal information, such as the access time, the location from which the access was made, and the frequency of visits of our website by the data subject. With each visit to our Internet site, such personal data, including the IP address of the Internet access used by the data subject, will be transmitted to Google in the United States of America. These personal data are stored by Google in the United States of America. Google may pass these personal data collected through the technical procedure to third parties. The data subject may, as stated above, prevent the setting of cookies through our website at any time by means of a corresponding adjustment of the web browser used and thus permanently deny the setting of cookies. Such an adjustment to the Internet browser used would also prevent Google Analytics from setting a cookie on the information technology system of the data subject. In addition, cookies already in use by Google Analytics may be deleted at any time via a web browser or other software programs. In addition, the data subject has the possibility of objecting to a collection of data that are generated by Google Analytics, which is related to the use of this website, as well as the processing of this data by Google and the chance to preclude any such. For this purpose, the data subject must download a browser add-on under the link https://tools.google.com/dlpage/gaoptout and install it. This browser add-on tells Google Analytics through a JavaScript, that any data and information about the visits of Internet pages may not be transmitted to Google Analytics. The installation of the browser add-ons is considered an objection by Google. If the information technology system of the data subject is later deleted, formatted, or newly installed, then the data subject must reinstall the browser add-ons to disable Google Analytics. If the browser add-on was uninstalled by the data subject or any other person who is attributable to their sphere of competence, or is disabled, it is possible to execute the reinstallation or reactivation of the browser add-ons. Further information and the applicable data protection provisions of Google may be retrieved under https://www.google.com/intl/en/policies/privacy/ and under http://www.google.com/analytics/terms/us.html. Google Analytics is further explained under the following Link https://www.google.com/analytics/. This Privacy Policy has been generated by the Privacy Policy Generator of the External Data Protection Officers that was developed in cooperation with the Media Law Lawyers from WBS-LAW. It has been modified by the controller to fit the specific needs of meshcaline.org.","title":"Privacy Policy"},{"location":"privacy/#privacy-policy","text":"The use of the Internet pages of meshcaline.org is possible without any indication of personal data; however, if a data subject wants to use special enterprise services via our website, processing of personal data could become necessary. If the processing of personal data is necessary and there is no statutory basis for such processing, we generally obtain consent from the data subject. The processing of personal data, such as the name, address, e-mail address, or telephone number of a data subject shall always be in line with the General Data Protection Regulation (GDPR), and in accordance with the country-specific data protection regulations applicable to meshcaline.org. By means of this data protection declaration, meshcaline.org would like to inform the general public of the nature, scope, and purpose of the personal data we collect, use and process. Furthermore, data subjects are informed, by means of this data protection declaration, of the rights to which they are entitled. As the controller, meshcaline.org has implemented numerous technical and organizational measures to ensure the most complete protection of personal data processed through this website. However, Internet-based data transmissions may in principle have security gaps, so absolute protection may not be guaranteed. For this reason, every data subject is free to transfer personal data to meshcaline.org via alternative means, e.g. by mail.","title":"Privacy Policy"},{"location":"privacy/#definitions","text":"The data protection declaration of meshcaline.org is based on the terms used by the European legislator for the adoption of the General Data Protection Regulation (GDPR). Our data protection declaration should be legible and understandable for the general public, as well as our customers and business partners. To ensure this, we would like to first explain the terminology used. In this data protection declaration, we use, inter alia, the following terms: Personal data Personal data means any information relating to an identified or identifiable natural person (\u201cdata subject\u201d). An identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person. Data subject Data subject is any identified or identifiable natural person, whose personal data is processed by the controller responsible for the processing. Processing Processing is any operation or set of operations which is performed on personal data or on sets of personal data, whether or not by automated means, such as collection, recording, organisation, structuring, storage, adaptation or alteration, retrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure or destruction. Restriction of processing Restriction of processing is the marking of stored personal data with the aim of limiting their processing in the future. Profiling Profiling means any form of automated processing of personal data consisting of the use of personal data to evaluate certain personal aspects relating to a natural person, in particular to analyse or predict aspects concerning that natural person's performance at work, economic situation, health, personal preferences, interests, reliability, behaviour, location or movements. Pseudonymisation Pseudonymisation is the processing of personal data in such a manner that the personal data can no longer be attributed to a specific data subject without the use of additional information, provided that such additional information is kept separately and is subject to technical and organisational measures to ensure that the personal data are not attributed to an identified or identifiable natural person. Controller or controller responsible for the processing Controller or controller responsible for the processing is the natural or legal person, public authority, agency or other body which, alone or jointly with others, determines the purposes and means of the processing of personal data; where the purposes and means of such processing are determined by Union or Member State law, the controller or the specific criteria for its nomination may be provided for by Union or Member State law. Processor Processor is a natural or legal person, public authority, agency or other body which processes personal data on behalf of the controller. Recipient Recipient is a natural or legal person, public authority, agency or another body, to which the personal data are disclosed, whether a third party or not. However, public authorities which may receive personal data in the framework of a particular inquiry in accordance with Union or Member State law shall not be regarded as recipients; the processing of those data by those public authorities shall be in compliance with the applicable data protection rules according to the purposes of the processing. Third party Third party is a natural or legal person, public authority, agency or body other than the data subject, controller, processor and persons who, under the direct authority of the controller or processor, are authorised to process personal data. Consent Consent of the data subject is any freely given, specific, informed and unambiguous indication of the data subject's wishes by which he or she, by a statement or by a clear affirmative action, signifies agreement to the processing of personal data relating to him or her.","title":"Definitions"},{"location":"privacy/#name-and-address-of-the-controller","text":"Controller for the purposes of the General Data Protection Regulation (GDPR), other data protection laws applicable in Member states of the European Union and other provisions related to data protection is: Andreas Schmidt Mittenwalder Str. 19 10961 Berlin Germany if( window.location.pathname==\"/about/\" ){ var t=\"org\", d=\"meshcaline\", n=\"meshcalero\"; var m=n+\"@\"+d+\".\"+t; document.write('Email: <a href=\"mailto:'+m+'\">'+m+'</a>');} Website: http://blog.meshcaline.org","title":"Name and address of the controller"},{"location":"privacy/#collection-of-general-data-and-information","text":"The website meshcaline.org collects a series of general data and information when a data subject or automated system calls up the website. This general data and information are stored in the server log files. Collected may be the browser types and versions used, the operating system used by the accessing system, the website from which an accessing system reaches our website (so-called referrers), the sub-websites, the date and time of access to the Internet site, an anonymized Internet protocol address (IP address), the Internet service provider of the accessing system, and any other similar data and information that may be used in the event of attacks on our information technology systems. When using these general data and information, meshcaline.org does not draw any conclusions about the data subject. Rather, this information is needed to deliver the content of our website correctly, optimize the content of our website as well as its advertisement, ensure the long-term viability of our information technology systems and website technology, and provide law enforcement authorities with the information necessary for criminal prosecution in case of a cyber-attack. Therefore, meshcaline.org analyzes anonymously collected data and information statistically, with the aim of increasing the data protection and data security of our website, and to ensure an optimal level of protection for the personal data we process. The anonymous data of the server log files are stored separately from all personal data provided by a data subject.","title":"Collection of general data and information"},{"location":"privacy/#routine-erasure-and-blocking-of-personal-data","text":"The data controller shall process and store the personal data of the data subject only for the period necessary to achieve the purpose of storage, or as far as this is granted by the European legislator or other legislators in laws or regulations to which the controller is subject to. If the storage purpose is not applicable, or if a storage period prescribed by the European legislator or another competent legislator expires, the personal data are routinely blocked or erased in accordance with legal requirements.","title":"Routine erasure and blocking of personal data"},{"location":"privacy/#rights-of-the-data-subject","text":"Right of confirmation Each data subject shall have the right granted by the European legislator to obtain from the controller the confirmation as to whether or not personal data concerning him or her are being processed. If a data subject wishes to avail himself of this right of confirmation, he or she may, at any time, contact any employee of the controller. Right of access Each data subject shall have the right granted by the European legislator to obtain from the controller free information about his or her personal data stored at any time and a copy of this information. Furthermore, the European directives and regulations grant the data subject access to the following information: * the purposes of the processing; * the categories of personal data concerned; * the recipients or categories of recipients to whom the personal data have been or will be disclosed, in particular recipients in third countries or international organisations; * where possible, the envisaged period for which the personal data will be stored, or, if not possible, the criteria used to determine that period; * the existence of the right to request from the controller rectification or erasure of personal data, or restriction of processing of personal data concerning the data subject, or to object to such processing; * the existence of the right to lodge a complaint with a supervisory authority; * where the personal data are not collected from the data subject, any available information as to their source; * the existence of automated decision-making, including profiling, referred to in Article 22(1) and (4) of the GDPR and, at least in those cases, meaningful information about the logic involved, as well as the significance and envisaged consequences of such processing for the data subject. Furthermore, the data subject shall have a right to obtain information as to whether personal data are transferred to a third country or to an international organisation. Where this is the case, the data subject shall have the right to be informed of the appropriate safeguards relating to the transfer. If a data subject wishes to avail himself of this right of access, he or she may, at any time, contact any employee of the controller. Right to rectification Each data subject shall have the right granted by the European legislator to obtain from the controller without undue delay the rectification of inaccurate personal data concerning him or her. Taking into account the purposes of the processing, the data subject shall have the right to have incomplete personal data completed, including by means of providing a supplementary statement. If a data subject wishes to exercise this right to rectification, he or she may, at any time, contact any employee of the controller. Right to erasure (Right to be forgotten) Each data subject shall have the right granted by the European legislator to obtain from the controller the erasure of personal data concerning him or her without undue delay, and the controller shall have the obligation to erase personal data without undue delay where one of the following grounds applies, as long as the processing is not necessary: The personal data are no longer necessary in relation to the purposes for which they were collected or otherwise processed. The data subject withdraws consent to which the processing is based according to point (a) of Article 6(1) of the GDPR, or point (a) of Article 9(2) of the GDPR, and where there is no other legal ground for the processing. The data subject objects to the processing pursuant to Article 21(1) of the GDPR and there are no overriding legitimate grounds for the processing, or the data subject objects to the processing pursuant to Article 21(2) of the GDPR. The personal data have been unlawfully processed. The personal data must be erased for compliance with a legal obligation in Union or Member State law to which the controller is subject. The personal data have been collected in relation to the offer of information society services referred to in Article 8(1) of the GDPR. If one of the aforementioned reasons applies, and a data subject wishes to request the erasure of personal data stored by meshcaline.org, he or she may, at any time, contact the controller. The controller shall promptly ensure that the erasure request is complied with immediately. Where the controller has made personal data public and is obliged pursuant to Article 17(1) to erase the personal data, the controller, taking account of available technology and the cost of implementation, shall take reasonable steps, including technical measures, to inform other controllers processing the personal data that the data subject has requested erasure by such controllers of any links to, or copy or replication of, those personal data, as far as processing is not required. meshcaline.org will arrange the necessary measures in individual cases. Right of restriction of processing Each data subject shall have the right granted by the European legislator to obtain from the controller restriction of processing where one of the following applies: * The accuracy of the personal data is contested by the data subject, for a period enabling the controller to verify the accuracy of the personal data. * The processing is unlawful and the data subject opposes the erasure of the personal data and requests instead the restriction of their use instead. * The controller no longer needs the personal data for the purposes of the processing, but they are required by the data subject for the establishment, exercise or defence of legal claims. * The data subject has objected to processing pursuant to Article 21(1) of the GDPR pending the verification whether the legitimate grounds of the controller override those of the data subject. If one of the aforementioned conditions is met, and a data subject wishes to request the restriction of the processing of personal data stored by meshcaline.org, he or she may at any time contact meshcaline.org. meshcaline.org will arrange the restriction of the processing. Right to data portability Each data subject shall have the right granted by the European legislator, to receive the personal data concerning him or her, which was provided to a controller, in a structured, commonly used and machine-readable format. He or she shall have the right to transmit those data to another controller without hindrance from the controller to which the personal data have been provided, as long as the processing is based on consent pursuant to point (a) of Article 6(1) of the GDPR or point (a) of Article 9(2) of the GDPR, or on a contract pursuant to point (b) of Article 6(1) of the GDPR, and the processing is carried out by automated means, as long as the processing is not necessary for the performance of a task carried out in the public interest or in the exercise of official authority vested in the controller. Furthermore, in exercising his or her right to data portability pursuant to Article 20(1) of the GDPR, the data subject shall have the right to have personal data transmitted directly from one controller to another, where technically feasible and when doing so does not adversely affect the rights and freedoms of others. In order to assert the right to data portability, the data subject may at any time contact the controller. Right to object Each data subject shall have the right granted by the European legislator to object, on grounds relating to his or her particular situation, at any time, to processing of personal data concerning him or her, which is based on point (e) or (f) of Article 6(1) of the GDPR. This also applies to profiling based on these provisions. meshcaline.org shall no longer process the personal data in the event of the objection, unless we can demonstrate compelling legitimate grounds for the processing which override the interests, rights and freedoms of the data subject, or for the establishment, exercise or defence of legal claims. If meshcaline.org processes personal data for direct marketing purposes, the data subject shall have the right to object at any time to processing of personal data concerning him or her for such marketing. This applies to profiling to the extent that it is related to such direct marketing. If the data subject objects to meshcaline.org to the processing for direct marketing purposes, meshcaline.org will no longer process the personal data for these purposes. In addition, the data subject has the right, on grounds relating to his or her particular situation, to object to processing of personal data concerning him or her by meshcaline.org for scientific or historical research purposes, or for statistical purposes pursuant to Article 89(1) of the GDPR, unless the processing is necessary for the performance of a task carried out for reasons of public interest. In order to exercise the right to object, the data subject may contact meshcaline.org. In addition, the data subject is free in the context of the use of information society services, and notwithstanding Directive 2002/58/EC, to use his or her right to object by automated means using technical specifications. Automated individual decision-making, including profiling Each data subject shall have the right granted by the European legislator not to be subject to a decision based solely on automated processing, including profiling, which produces legal effects concerning him or her, or similarly significantly affects him or her, as long as the decision (1) is not is necessary for entering into, or the performance of, a contract between the data subject and a data controller, or (2) is not authorised by Union or Member State law to which the controller is subject and which also lays down suitable measures to safeguard the data subject's rights and freedoms and legitimate interests, or (3) is not based on the data subject's explicit consent. If the decision (1) is necessary for entering into, or the performance of, a contract between the data subject and a data controller, or (2) it is based on the data subject's explicit consent, meshcaline.org shall implement suitable measures to safeguard the data subject's rights and freedoms and legitimate interests, at least the right to obtain human intervention on the part of the controller, to express his or her point of view and contest the decision. If the data subject wishes to exercise the rights concerning automated individual decision-making, he or she may, at any time, contact meshcaline.org. Right to withdraw data protection consent Each data subject shall have the right granted by the European legislator to withdraw his or her consent to processing of his or her personal data at any time. If the data subject wishes to exercise the right to withdraw the consent, he or she may, at any time, contact meshcaline.org.","title":"Rights of the data subject"},{"location":"privacy/#legal-basis-for-the-processing","text":"Art. 6(1) lit. a GDPR serves as the legal basis for processing operations for which we obtain consent for a specific processing purpose. If the processing of personal data is necessary for the performance of a contract to which the data subject is party, as is the case, for example, when processing operations are necessary for the supply of goods or to provide any other service, the processing is based on Article 6(1) lit. b GDPR. The same applies to such processing operations which are necessary for carrying out pre-contractual measures, for example in the case of inquiries concerning our products or services. Is our company subject to a legal obligation by which processing of personal data is required, such as for the fulfillment of tax obligations, the processing is based on Art. 6(1) lit. c GDPR. In rare cases, the processing of personal data may be necessary to protect the vital interests of the data subject or of another natural person. This would be the case, for example, if a visitor were injured in our company and his name, age, health insurance data or other vital information would have to be passed on to a doctor, hospital or other third party. Then the processing would be based on Art. 6(1) lit. d GDPR. Finally, processing operations could be based on Article 6(1) lit. f GDPR. This legal basis is used for processing operations which are not covered by any of the abovementioned legal grounds, if processing is necessary for the purposes of the legitimate interests pursued by our company or by a third party, except where such interests are overridden by the interests or fundamental rights and freedoms of the data subject which require protection of personal data. Such processing operations are particularly permissible because they have been specifically mentioned by the European legislator. He considered that a legitimate interest could be assumed if the data subject is a client of the controller (Recital 47 Sentence 2 GDPR).","title":"Legal basis for the processing"},{"location":"privacy/#the-legitimate-interests-pursued-by-the-controller-or-by-a-third-party","text":"Where the processing of personal data is based on Article 6(1) lit. f GDPR our legitimate interest is to being able to offer meshcaline.org to interested readers and continuously improve the service.","title":"The legitimate interests pursued by the controller or by a third party"},{"location":"privacy/#period-for-which-the-personal-data-will-be-stored","text":"The criteria used to determine the period of storage of personal data is the respective statutory retention period. After expiration of that period, the corresponding data is routinely deleted, as long as it is no longer necessary for the fulfillment of the contract or the initiation of a contract.","title":"Period for which the personal data will be stored"},{"location":"privacy/#provision-of-personal-data-as-statutory-or-contractual-requirement-requirement-necessary-to-enter-into-a-contract-obligation-of-the-data-subject-to-provide-the-personal-data-possible-consequences-of-failure-to-provide-such-data","text":"We clarify that the provision of personal data is partly required by law (e.g. tax regulations) or can also result from contractual provisions (e.g. information on the contractual partner). Sometimes it may be necessary to conclude a contract that the data subject provides us with personal data, which must subsequently be processed by us. The data subject is, for example, obliged to provide us with personal data when our company signs a contract with him or her. The non-provision of the personal data would have the consequence that the contract with the data subject could not be concluded. Before personal data is provided by the data subject, the data subject must contact the controller. The controller clarifies to the data subject whether the provision of the personal data is required by law or contract or is necessary for the conclusion of the contract, whether there is an obligation to provide the personal data and the consequences of non-provision of the personal data.","title":"Provision of personal data as statutory or contractual requirement; Requirement necessary to enter into a contract; Obligation of the data subject to provide the personal data; possible consequences of failure to provide such data"},{"location":"privacy/#existence-of-automated-decision-making","text":"As a responsible controller, we do not use automatic decision-making or profiling.","title":"Existence of automated decision-making"},{"location":"privacy/#3rd-parties","text":"","title":"3rd Parties"},{"location":"privacy/#hosting","text":"The hosting services we use provide the following services: infrastructure and platform services, computing capacity, storage and database services, security and technical maintenance services we use to operate this online service. Here we, or our hosting provider, process inventory data, contact data, content data, contract data, usage data, meta and communication data of customers, interested parties and visitors to this online offer on the basis of our legitimate interests in an efficient and secure provision of this online offer acc. art. 6 para. 1 lit. f GDPR in combination with art. 28 GDPR.","title":"Hosting"},{"location":"privacy/#disqus-comment-function","text":"Based on our legitimate interestson on an efficient, secure and user-friendly comment management acc. Art. 6 para. 1 lit. f. GDPR we rely on the commenting service DISQUS offered by DISQUS, Inc., 301 Howard St, Floor 3, San Francisco, California 94105, USA. DISQUS is certified under the Privacy Shield Agreement, which provides a guarantee to comply with European privacy legislation: https://www.privacyshield.gov/participant?id=a2zt0000000TRkEAAW&status=Active To use the DISQUS comment feature, users can log in using their own DISQUS user account or existing social media accounts (e.g., OpenID, Facebook, Twitter, or Google). Here, the user credentials are obtained from the platforms through DISQS. It is also possible to use the DISQUS comment feature as a guest without creating or using user accounts with DISQUS or any of the social media providers listed. We only embed DISQUS with its features in our website, and we can influence the comments of the users. However, users enter into a direct contractual relationship with DISQUS, in which DISQS processes users 'comments and acts as point of contact for any deletion of users' data. We refer to the privacy policy of DISQUS: https://help.disqus.com/terms-and-policies/disqus-privacy-policy and also inform the users that they can assume that DISQUS not only stores the comment content but also their IP address and the time of the comment, as well stores cookies on the computers of users and can use this data to display advertising. However, users may object to the processing of their data to display ads: https://disqus.com/data-sharing-settings","title":"DISQUS Comment Function"},{"location":"privacy/#google-analytics","text":"On this website, the controller has integrated the component of Google Analytics (with the anonymizer function). Google Analytics is a web analytics service. Web analytics is the collection, gathering, and analysis of data about the behavior of visitors to websites. A web analysis service collects, inter alia, data about the website from which a person has come (the so-called referrer), which sub-pages were visited, or how often and for what duration a sub-page was viewed. Web analytics are mainly used for the optimization of a website and in order to carry out a cost-benefit analysis of Internet advertising. The operator of the Google Analytics component is Google Inc., 1600 Amphitheatre Pkwy, Mountain View, CA 94043-1351, United States. For the web analytics through Google Analytics the controller uses IP Anonymization in Analytics . By means of this feature the IP address of the Internet connection of the data subject is abridged by Google and anonymised when accessing our websites from a Member State of the European Union or another Contracting State to the Agreement on the European Economic Area. The purpose of the Google Analytics component is to analyze the traffic on our website. Google uses the collected data and information, inter alia, to evaluate the use of our website and to provide online reports, which show the activities on our websites, and to provide other services concerning the use of our Internet site for us. Google Analytics places a cookie on the information technology system of the data subject. The definition of cookies is explained above. With the setting of the cookie, Google is enabled to analyze the use of our website. With each call-up to one of the individual pages of this Internet site, which is operated by the controller and into which a Google Analytics component was integrated, the Internet browser on the information technology system of the data subject will automatically submit data through the Google Analytics component for the purpose of online advertising and the settlement of commissions to Google. During the course of this technical procedure, the enterprise Google gains knowledge of personal information, such as the IP address of the data subject, which serves Google, inter alia, to understand the origin of visitors and clicks, and subsequently create commission settlements. The cookie is used to store personal information, such as the access time, the location from which the access was made, and the frequency of visits of our website by the data subject. With each visit to our Internet site, such personal data, including the IP address of the Internet access used by the data subject, will be transmitted to Google in the United States of America. These personal data are stored by Google in the United States of America. Google may pass these personal data collected through the technical procedure to third parties. The data subject may, as stated above, prevent the setting of cookies through our website at any time by means of a corresponding adjustment of the web browser used and thus permanently deny the setting of cookies. Such an adjustment to the Internet browser used would also prevent Google Analytics from setting a cookie on the information technology system of the data subject. In addition, cookies already in use by Google Analytics may be deleted at any time via a web browser or other software programs. In addition, the data subject has the possibility of objecting to a collection of data that are generated by Google Analytics, which is related to the use of this website, as well as the processing of this data by Google and the chance to preclude any such. For this purpose, the data subject must download a browser add-on under the link https://tools.google.com/dlpage/gaoptout and install it. This browser add-on tells Google Analytics through a JavaScript, that any data and information about the visits of Internet pages may not be transmitted to Google Analytics. The installation of the browser add-ons is considered an objection by Google. If the information technology system of the data subject is later deleted, formatted, or newly installed, then the data subject must reinstall the browser add-ons to disable Google Analytics. If the browser add-on was uninstalled by the data subject or any other person who is attributable to their sphere of competence, or is disabled, it is possible to execute the reinstallation or reactivation of the browser add-ons. Further information and the applicable data protection provisions of Google may be retrieved under https://www.google.com/intl/en/policies/privacy/ and under http://www.google.com/analytics/terms/us.html. Google Analytics is further explained under the following Link https://www.google.com/analytics/. This Privacy Policy has been generated by the Privacy Policy Generator of the External Data Protection Officers that was developed in cooperation with the Media Law Lawyers from WBS-LAW. It has been modified by the controller to fit the specific needs of meshcaline.org.","title":"Google Analytics"},{"location":"rest-in-peace/","text":"REST in Peace Before actually describing the API design ideas of meshcaline , I will start with a subject that became more and more apparent, the longer I worked with our users on improvements of our API design: Let's stop talking about REST(ful) API design! One of the most misused terms in API development -- if not software development in general -- is REST or RESTful. Most likely only few people have read or fully understood the REST architecture style as described in Roy Fielding's dissertation published back in 2000 (and I can't claim either). Looking back into the early 2000s, after the dust, dispersed by the hype around XML, XML-RPC and its successor SOAP and the ws-posse had settled, many developers where just fed up with interface generators and complex, incomplete and incompatible standard implementations. They wanted something more lightweight, tangible and robust for their client/server communication, especially when the client was a web browser. Most developers found it immediately appealing to apply the same mechanisms commonly used for passing values between dynamic web pages also for they mechanisms they used to interface with dynamic data sources on the server-side. Nobody really cared about the full concept behind Roy Fielding's work, but everyone was just happy that there was now a name for something they already were doing and that did not come with so much overhead and pain. The fact that the term REST was known to be be the underlying concept of the web as such gave this approach an accolade, and nipped any discussion in the bud whether SOAP or anything else would be a valid or even superior alternative. And with the next hype around web 2.0, the acronym REST, originally a quite technical term, became quickly a marketing buzzword -- although barely anyone using it could explain what \"Representational State Transfer\" really meant. With the rise of successful public web service offerings, and more and more of them claiming to be RESTful, in 2008 Roy Fielding felt compelled to state six rules which an API must fulfill before calling itself REST API. Unfortunately, at that time the term REST was already too important for the marketing buzz, so the inventor of the term had no chance to prevent the ongoing misuse. But even today, nearly six years after the rules got published, and many successful web service APIs in the market, I am not aware of any popular web service API that fulfills all of those rules. Also most books about REST APIs -- even the ones I find great -- just pick up some of the aspects. So there must be something in those rules that either API developers don't like or at least don't see that they create value for their API. Alternatively the potential users of such APIs didn't like those API designs and then those APIs never get popular. If your goal is a successful API you better understand how the rules you are following supports you on that. Although I can't claim that my interpretation of the rules is exactly what Roy Fielding had in mind, here comes my analysis: Rule 1: Protocol Dependencies A REST API should not be dependent on any single communication protocol, though its successful mapping to a given protocol may be dependent on the availability of metadata, choice of methods, etc. In general, any protocol element that uses a URI for identification must allow any URI scheme to be used for the sake of that identification [Failure here implies that identification is not separated from interaction.] I would guess barely any API designer spends time on the first sentence of this rule, simply because what most of them want to build are web service APIs, so HTTP as underlying protocol (and its secure sister HTTPS) is set. So why should a developer spend time thinking about the possibility to map his API to some other protocol. While it may be an nice academic exercise to also think about how to offer an API via SMTP (even SOAP in theory supported that approach too), in most cases it does not create real business value. And so this becomes the first rule API designers are more than willing to drop. The second sentence is often read over although it contains an important conceptual detail: the identity of something referred to in an API protocol should not be bound to the accessibility of that object. The primary purpose of identifiers is to give an object a unique name(space), not necessarily to provide information how to access that object. Many objects might only be accessible in the context of other resources, so you can't provide means to access them directly. But you might still want to identify the object in the context of various resources it is embedded in. Unfortunately, many APIs ignore that rule. Instead of clearly separating between hypertext elements and identifiers, many API designs follow a paradigm that I call \"Object-relational mapping over HTTP\": Foreign keys of an underlying database model get expressed as \"executable\" URL, and therefore the separation of identity and accessibility is considered an avoidable overhead. Rule 2: Protocol Extensions A REST API should not contain any changes to the communication protocols aside from filling-out or fixing the details of underspecified bits of standard protocols, such as HTTP\u2019s PATCH method or Link header field. Workarounds for broken implementations (such as those browsers stupid enough to believe that HTML defines HTTP\u2019s method set) should be defined separately, or at least in appendices, with an expectation that the workaround will eventually be obsolete. [Failure here implies that the resource interfaces are object-specific, not generic.]. While most API designs do not have to modify the underlying protocol, the question when a protocol is underspecified may be a quite subjective judgment. Just check how many IETF RFC exist that introduce additional HTTP headers, which are only important for a very specific domain of services. When the communication protocol your API is using is anyway set to HTTP(S), and the best way to get a problem solved for your users is by introducing some private header, then being pragmatic might be a better solution than following the book and then miss to meet the needs of your users. I think as long as the extension is a generic one (at least in the context of your complete API, not just for an individual resource) then I'm willing to relax the rule to: don't do this, unless you know why you do this. Rule 3: Media Types A REST API should spend almost all of its descriptive effort in defining the media type(s) used for representing resources and driving application state, or in defining extended relation names and/or hypertext-enabled mark-up for existing standard media types. Any effort spent describing what methods to use on what URIs of interest should be entirely defined within the scope of the processing rules for a media type (and, in most cases, already defined by existing media types). [Failure here implies that out-of-band information is driving interaction instead of hypertext.] I think nobody debates that defining media types is the most important part of any API definition. Unfortunately not every aspect of this rule is precisely defined and leaves room for interpretation and dispute. To me is it not clear what \"processing rules for a media type\" are allowed to contain. Just two examples for details that I find unclear, although important: The rule mentions \"methods\" should be defined in the processing rules of a media type, but I am not sure how this should work: A media type \"image/jpeg\" of a given resource does not define if the corresponding API that delivers such a media type in a response of a GET method also supports a PUT method on the same resource. It does this not define precisely, if such processing rules might allow parameters for a given resource: Assume you define a new media type for \"adjustable images\", that allows you to modify various parameters of a returned image (transparency, size, dpi, background color, cropping...). Would it be valid to specify in the processing rules, that you can retrieve a variant of such an \"adjustable image\" by adding various parameters to such the URL of such a resource, or would it require specific hypertext controls in the link to such a resource which would allow you to specify such parameters. I fully understand why REST propagates the avoidance of \"out of band\" information and how this contributes to the success of the web. But the real power of this approach comes into play when you define a generic protocol (where you may have many server implementations and more or less generic clients using them). But this is not what drives most developers when they design an API for their product. It is not their primary goal to allow their users to also use other API products, unless they mimic another popular API to entice its customers away to their solution. To make it worse: The primary customers (the application developers) want an API that solves their specific problem in a simple, long-term-stable way. They are very happy with some \"hard coded interaction behavior\" in their code, as they believe it is the responsibility of the API provider to maintain that behavior -- and believe me I had many discussions with app developers on that topic. Rule 4: Resource Addressing A REST API must not define fixed resource names or hierarchies (an obvious coupling of client and server). Servers must have the freedom to control their own namespace. Instead, allow servers to instruct clients on how to construct appropriate URIs, such as is done in HTML forms and URI templates, by defining those instructions within media types and link relations. [Failure here implies that clients are assuming a resource structure due to out-of band information, such as a domain-specific standard, which is the data-oriented equivalent to RPC's functional coupling]. This again is a tricky rule. From an API developer's point of view, it might be understandable that you want to retain the right to change resource names whenever you have a need for it. On the other hand, using hard wired URI templates in their code is something all applications developers are familiar with and tend to prefer. Just one anecdote from our API project: For some time we did support both, a nice, read to use hypertext URI field in our response, but also a documented URI template that allowed the construction of the actual URL based on the template and other attributes found in the response. And guess what: Nearly 100% of the app developers did use the URI template approach, even when using just the full URL for the response would have been far simpler. Also forms (or URI templates, which for me is just a low-level variant of a form) as part of the response may sound like a good idea in the first place (\"if it works for browsers then is must also work for API)\". But keep in mind that forms in HTML do work because they get primarily used to construct a UI for a human beings. Those \"consumers\" are capable to derive what to enter into the various fields from the context in which the form is presented and the meta-information displayed long with the form (e.g. labels). Humans will find the source for the information they are supposed to provide in the fields. Even when things are not completely clear, we are capable to try out and test how the system behaves and re-try if the resulting behavior doesn't match expectations. That's not the way app developers want to implement the code that interacts with your API. Developers want to know in advance (at coding time) what information is required in what context and then either ask their own users to provide this information (by building a form in their UI), or derive the information from the environment or state the application is in. Once again I believe that this rule helped the web becoming as successful as it is. But if applied to a specific API product, you might risk to meet the expectations of your customers, and by that fail on the market, unless you are a monopolist who can enforce the usage. Rule 5: Typed Resources A REST API should never have \u201ctyped\u201d resources that are significant to the client. Specification authors may use resource types for describing server implementation behind the interface, but those types must be irrelevant and invisible to the client. The only types that are significant to a client are the current representation\u2019s media type and standardized relation names. [ditto] If the previous rule already risks that your API design makes your customers unhappy, that one definitively will. Not only does it not meet the mental model of a developer who calls an API for some specific purpose, and for that the API is expected to respond with some \"type\" of result. In addition, every API call has a \"price\". Even when you do not charge for the usage of your API directly, the client application will have to fire the request, wait for the response and then process the response accordingly. So the currency of the price all apps will have to pay is time, and this usually has a direct impact on the responsiveness of their application. Therefore your users don't want to access a resource and wait for the response just to then learn, that either the server can't deliver what they would like, or they can't process what the server responds. They want to know upfront which methods a specific resource supports and what the response will return -- ideally at coding time (to make if simple for the developer) but at least before they decide to fire a request (to avoid negative impact for the app). Rule 6: Service Discovery A REST API should be entered with no prior knowledge beyond the initial URI (bookmark) and set of standardized media types that are appropriate for the intended audience (i.e., expected to be understood by any client that might use the API). From that point on, all application state transitions must be driven by client selection of server-provided choices that are present in the received representations or implied by the user\u2019s manipulation of those representations. The transitions may be determined (or limited by) the client\u2019s knowledge of media types and resource communication mechanisms, both of which may be improved on-the-fly (e.g., code-on-demand). [Failure here implies that out-of-band information is driving interaction instead of hypertext.] Also this rule has questionable value for an API. Your users want to use certain features of your API. Usually they expect to use some URI template from the documentation. If you would follow this rule, your users the would have to fetch the initial URL and then scan the result for some link relation type identifying, the features they want to use. While the advantage for you as API developer is obvious, you must also find an answer for the question how this helps your users. If the flexibility it brings to you doesn't introduce also advantages for your users, then they will not like it, because it (a) requires an avoidable request and (b) adds complexity to the app code -- they first have to parse the response in order to find which relation types are available. And they would even have to prepare for the case, where the relation type they need is no longer available or the response offers multiple options to choose from. When your API does provide functionality beyond \"simple links\", then you would have to embed into the initial response also the forms needed to construct the request, with all the problems coming with forms described above. Conclusion So where did this analysis lead me to? I doubt I will ever design an API that honors all or even most of Roy Fielding's rules. Also I doubt a commercial API product will become successful if it tries to. Although I am deeply convinced of the advantage of the HATEOAS constraint for API, I am equally convinced that a prosperous commercial API product requires a different design paradigm than the one derived from the REST rules. At the same time I believe it is important to take definitions serious and not to dilute the work of its creator by ignoring the parts you don't like or that do not fit. What is the purpose of a definition, if people ignore it? You would not like an integer variable to contain a float value, because a developer thought that the restriction that came with integer where not appropriate in this context and this little dot can't really hurt. You should only call your API RESTful, when you are willing to accept all the rules. Let the marketing guys put a REST tag to your offering if it helps them promoting your product, but as developers we should be precise. Realizing that playing RESTful according to Roy Fielding's rules in not my type of game, I decided that its worth to write down the concepts I believe are more appropriate for API based products -- and so the idea for this blog started. But if it is not REST, how should I call the ideas? Without a name you can't precisely state what you are talking about. Initially my ideas for names where still attached to REST, like RESTless, RESTignation, etc. But then I realized that those names seem to indicate that there is something wrong with REST, that needs improvement. And that's definitively not what I want to say. REST is one of the most important architectural concepts in today's IT landscape and this will remain. It's just not the right tool for everything. So I had to find name independent from REST and came up with meshcaline .","title":"REST in Peace"},{"location":"rest-in-peace/#rest-in-peace","text":"Before actually describing the API design ideas of meshcaline , I will start with a subject that became more and more apparent, the longer I worked with our users on improvements of our API design:","title":"REST in Peace"},{"location":"rest-in-peace/#lets-stop-talking-about-restful-api-design","text":"One of the most misused terms in API development -- if not software development in general -- is REST or RESTful. Most likely only few people have read or fully understood the REST architecture style as described in Roy Fielding's dissertation published back in 2000 (and I can't claim either). Looking back into the early 2000s, after the dust, dispersed by the hype around XML, XML-RPC and its successor SOAP and the ws-posse had settled, many developers where just fed up with interface generators and complex, incomplete and incompatible standard implementations. They wanted something more lightweight, tangible and robust for their client/server communication, especially when the client was a web browser. Most developers found it immediately appealing to apply the same mechanisms commonly used for passing values between dynamic web pages also for they mechanisms they used to interface with dynamic data sources on the server-side. Nobody really cared about the full concept behind Roy Fielding's work, but everyone was just happy that there was now a name for something they already were doing and that did not come with so much overhead and pain. The fact that the term REST was known to be be the underlying concept of the web as such gave this approach an accolade, and nipped any discussion in the bud whether SOAP or anything else would be a valid or even superior alternative. And with the next hype around web 2.0, the acronym REST, originally a quite technical term, became quickly a marketing buzzword -- although barely anyone using it could explain what \"Representational State Transfer\" really meant. With the rise of successful public web service offerings, and more and more of them claiming to be RESTful, in 2008 Roy Fielding felt compelled to state six rules which an API must fulfill before calling itself REST API. Unfortunately, at that time the term REST was already too important for the marketing buzz, so the inventor of the term had no chance to prevent the ongoing misuse. But even today, nearly six years after the rules got published, and many successful web service APIs in the market, I am not aware of any popular web service API that fulfills all of those rules. Also most books about REST APIs -- even the ones I find great -- just pick up some of the aspects. So there must be something in those rules that either API developers don't like or at least don't see that they create value for their API. Alternatively the potential users of such APIs didn't like those API designs and then those APIs never get popular. If your goal is a successful API you better understand how the rules you are following supports you on that. Although I can't claim that my interpretation of the rules is exactly what Roy Fielding had in mind, here comes my analysis:","title":"Let's stop talking about REST(ful) API design!"},{"location":"rest-in-peace/#rule-1-protocol-dependencies","text":"A REST API should not be dependent on any single communication protocol, though its successful mapping to a given protocol may be dependent on the availability of metadata, choice of methods, etc. In general, any protocol element that uses a URI for identification must allow any URI scheme to be used for the sake of that identification [Failure here implies that identification is not separated from interaction.] I would guess barely any API designer spends time on the first sentence of this rule, simply because what most of them want to build are web service APIs, so HTTP as underlying protocol (and its secure sister HTTPS) is set. So why should a developer spend time thinking about the possibility to map his API to some other protocol. While it may be an nice academic exercise to also think about how to offer an API via SMTP (even SOAP in theory supported that approach too), in most cases it does not create real business value. And so this becomes the first rule API designers are more than willing to drop. The second sentence is often read over although it contains an important conceptual detail: the identity of something referred to in an API protocol should not be bound to the accessibility of that object. The primary purpose of identifiers is to give an object a unique name(space), not necessarily to provide information how to access that object. Many objects might only be accessible in the context of other resources, so you can't provide means to access them directly. But you might still want to identify the object in the context of various resources it is embedded in. Unfortunately, many APIs ignore that rule. Instead of clearly separating between hypertext elements and identifiers, many API designs follow a paradigm that I call \"Object-relational mapping over HTTP\": Foreign keys of an underlying database model get expressed as \"executable\" URL, and therefore the separation of identity and accessibility is considered an avoidable overhead.","title":"Rule 1: Protocol Dependencies"},{"location":"rest-in-peace/#rule-2-protocol-extensions","text":"A REST API should not contain any changes to the communication protocols aside from filling-out or fixing the details of underspecified bits of standard protocols, such as HTTP\u2019s PATCH method or Link header field. Workarounds for broken implementations (such as those browsers stupid enough to believe that HTML defines HTTP\u2019s method set) should be defined separately, or at least in appendices, with an expectation that the workaround will eventually be obsolete. [Failure here implies that the resource interfaces are object-specific, not generic.]. While most API designs do not have to modify the underlying protocol, the question when a protocol is underspecified may be a quite subjective judgment. Just check how many IETF RFC exist that introduce additional HTTP headers, which are only important for a very specific domain of services. When the communication protocol your API is using is anyway set to HTTP(S), and the best way to get a problem solved for your users is by introducing some private header, then being pragmatic might be a better solution than following the book and then miss to meet the needs of your users. I think as long as the extension is a generic one (at least in the context of your complete API, not just for an individual resource) then I'm willing to relax the rule to: don't do this, unless you know why you do this.","title":"Rule 2: Protocol Extensions"},{"location":"rest-in-peace/#rule-3-media-types","text":"A REST API should spend almost all of its descriptive effort in defining the media type(s) used for representing resources and driving application state, or in defining extended relation names and/or hypertext-enabled mark-up for existing standard media types. Any effort spent describing what methods to use on what URIs of interest should be entirely defined within the scope of the processing rules for a media type (and, in most cases, already defined by existing media types). [Failure here implies that out-of-band information is driving interaction instead of hypertext.] I think nobody debates that defining media types is the most important part of any API definition. Unfortunately not every aspect of this rule is precisely defined and leaves room for interpretation and dispute. To me is it not clear what \"processing rules for a media type\" are allowed to contain. Just two examples for details that I find unclear, although important: The rule mentions \"methods\" should be defined in the processing rules of a media type, but I am not sure how this should work: A media type \"image/jpeg\" of a given resource does not define if the corresponding API that delivers such a media type in a response of a GET method also supports a PUT method on the same resource. It does this not define precisely, if such processing rules might allow parameters for a given resource: Assume you define a new media type for \"adjustable images\", that allows you to modify various parameters of a returned image (transparency, size, dpi, background color, cropping...). Would it be valid to specify in the processing rules, that you can retrieve a variant of such an \"adjustable image\" by adding various parameters to such the URL of such a resource, or would it require specific hypertext controls in the link to such a resource which would allow you to specify such parameters. I fully understand why REST propagates the avoidance of \"out of band\" information and how this contributes to the success of the web. But the real power of this approach comes into play when you define a generic protocol (where you may have many server implementations and more or less generic clients using them). But this is not what drives most developers when they design an API for their product. It is not their primary goal to allow their users to also use other API products, unless they mimic another popular API to entice its customers away to their solution. To make it worse: The primary customers (the application developers) want an API that solves their specific problem in a simple, long-term-stable way. They are very happy with some \"hard coded interaction behavior\" in their code, as they believe it is the responsibility of the API provider to maintain that behavior -- and believe me I had many discussions with app developers on that topic.","title":"Rule 3: Media Types"},{"location":"rest-in-peace/#rule-4-resource-addressing","text":"A REST API must not define fixed resource names or hierarchies (an obvious coupling of client and server). Servers must have the freedom to control their own namespace. Instead, allow servers to instruct clients on how to construct appropriate URIs, such as is done in HTML forms and URI templates, by defining those instructions within media types and link relations. [Failure here implies that clients are assuming a resource structure due to out-of band information, such as a domain-specific standard, which is the data-oriented equivalent to RPC's functional coupling]. This again is a tricky rule. From an API developer's point of view, it might be understandable that you want to retain the right to change resource names whenever you have a need for it. On the other hand, using hard wired URI templates in their code is something all applications developers are familiar with and tend to prefer. Just one anecdote from our API project: For some time we did support both, a nice, read to use hypertext URI field in our response, but also a documented URI template that allowed the construction of the actual URL based on the template and other attributes found in the response. And guess what: Nearly 100% of the app developers did use the URI template approach, even when using just the full URL for the response would have been far simpler. Also forms (or URI templates, which for me is just a low-level variant of a form) as part of the response may sound like a good idea in the first place (\"if it works for browsers then is must also work for API)\". But keep in mind that forms in HTML do work because they get primarily used to construct a UI for a human beings. Those \"consumers\" are capable to derive what to enter into the various fields from the context in which the form is presented and the meta-information displayed long with the form (e.g. labels). Humans will find the source for the information they are supposed to provide in the fields. Even when things are not completely clear, we are capable to try out and test how the system behaves and re-try if the resulting behavior doesn't match expectations. That's not the way app developers want to implement the code that interacts with your API. Developers want to know in advance (at coding time) what information is required in what context and then either ask their own users to provide this information (by building a form in their UI), or derive the information from the environment or state the application is in. Once again I believe that this rule helped the web becoming as successful as it is. But if applied to a specific API product, you might risk to meet the expectations of your customers, and by that fail on the market, unless you are a monopolist who can enforce the usage.","title":"Rule 4: Resource Addressing"},{"location":"rest-in-peace/#rule-5-typed-resources","text":"A REST API should never have \u201ctyped\u201d resources that are significant to the client. Specification authors may use resource types for describing server implementation behind the interface, but those types must be irrelevant and invisible to the client. The only types that are significant to a client are the current representation\u2019s media type and standardized relation names. [ditto] If the previous rule already risks that your API design makes your customers unhappy, that one definitively will. Not only does it not meet the mental model of a developer who calls an API for some specific purpose, and for that the API is expected to respond with some \"type\" of result. In addition, every API call has a \"price\". Even when you do not charge for the usage of your API directly, the client application will have to fire the request, wait for the response and then process the response accordingly. So the currency of the price all apps will have to pay is time, and this usually has a direct impact on the responsiveness of their application. Therefore your users don't want to access a resource and wait for the response just to then learn, that either the server can't deliver what they would like, or they can't process what the server responds. They want to know upfront which methods a specific resource supports and what the response will return -- ideally at coding time (to make if simple for the developer) but at least before they decide to fire a request (to avoid negative impact for the app).","title":"Rule 5: Typed Resources"},{"location":"rest-in-peace/#rule-6-service-discovery","text":"A REST API should be entered with no prior knowledge beyond the initial URI (bookmark) and set of standardized media types that are appropriate for the intended audience (i.e., expected to be understood by any client that might use the API). From that point on, all application state transitions must be driven by client selection of server-provided choices that are present in the received representations or implied by the user\u2019s manipulation of those representations. The transitions may be determined (or limited by) the client\u2019s knowledge of media types and resource communication mechanisms, both of which may be improved on-the-fly (e.g., code-on-demand). [Failure here implies that out-of-band information is driving interaction instead of hypertext.] Also this rule has questionable value for an API. Your users want to use certain features of your API. Usually they expect to use some URI template from the documentation. If you would follow this rule, your users the would have to fetch the initial URL and then scan the result for some link relation type identifying, the features they want to use. While the advantage for you as API developer is obvious, you must also find an answer for the question how this helps your users. If the flexibility it brings to you doesn't introduce also advantages for your users, then they will not like it, because it (a) requires an avoidable request and (b) adds complexity to the app code -- they first have to parse the response in order to find which relation types are available. And they would even have to prepare for the case, where the relation type they need is no longer available or the response offers multiple options to choose from. When your API does provide functionality beyond \"simple links\", then you would have to embed into the initial response also the forms needed to construct the request, with all the problems coming with forms described above.","title":"Rule 6: Service Discovery"},{"location":"rest-in-peace/#conclusion","text":"So where did this analysis lead me to? I doubt I will ever design an API that honors all or even most of Roy Fielding's rules. Also I doubt a commercial API product will become successful if it tries to. Although I am deeply convinced of the advantage of the HATEOAS constraint for API, I am equally convinced that a prosperous commercial API product requires a different design paradigm than the one derived from the REST rules. At the same time I believe it is important to take definitions serious and not to dilute the work of its creator by ignoring the parts you don't like or that do not fit. What is the purpose of a definition, if people ignore it? You would not like an integer variable to contain a float value, because a developer thought that the restriction that came with integer where not appropriate in this context and this little dot can't really hurt. You should only call your API RESTful, when you are willing to accept all the rules. Let the marketing guys put a REST tag to your offering if it helps them promoting your product, but as developers we should be precise. Realizing that playing RESTful according to Roy Fielding's rules in not my type of game, I decided that its worth to write down the concepts I believe are more appropriate for API based products -- and so the idea for this blog started. But if it is not REST, how should I call the ideas? Without a name you can't precisely state what you are talking about. Initially my ideas for names where still attached to REST, like RESTless, RESTignation, etc. But then I realized that those names seem to indicate that there is something wrong with REST, that needs improvement. And that's definitively not what I want to say. REST is one of the most important architectural concepts in today's IT landscape and this will remain. It's just not the right tool for everything. So I had to find name independent from REST and came up with meshcaline .","title":"Conclusion"}]}